{
  
    
        "post0": {
            "title": "Notes on writing a good user story",
            "content": "What is a user story? . It is a simplified high level description of a user requirement written from end user’s perspective. . How are user stories derived? . User stories are derived from EPIC and user persona . EPIC: It is a big chunk of work that has one common objective.It could be a feature, customer request or a business requirement. An epic usually takes more than a sprint to complete. . User Persona: A user persona is a fictional representation of the ideal user. A persona is based on user research and it incorporates the needs, goals and observed behavioral patters of the target audience. . What are the components of a good user story? . A good user story should have two main components. . User story equation: As a {User-Persona}, I want to {Some feature}, so that {Reason of using the feature}. It should be: short | simple | describes only one piece of functionality. If needed, break it down to 2 user stories | . | Acceptance Criteria: This list the activities that needs to be performed to provide value to the user. This helps a team to understand the value of the story and to set expectations as to when a team should consider something done, It should include: negative scenarios of the functionality | functional and non-functional user cases | performance concerns and guidelines | what feature intends to do | end to end user flow | impact of user story to other features | UX concerns | . | How to know if a user story is ready? . A complete user story should be clear, feasible and testable. . Clear: It includes what is needed by the user . Feasible: It should not be too big and can fit in a sprint . Testable: A confirmation can be provided post story completion about the correctness of the user story . Example of an User story . Epic: Developing online booking platform for cars. . User Story: As a car buyer I would like to see the latest price and offers so that I can make a decision to buy a car. . Acceptance Criteria: . Latest car price is to be shown | Offer is to be shown in terms of % Saving (10% off) | % Saving should be shown in bold and vibrant colors | In case of no offer, the car listing should not be visible | Car listing widget should be placed at a prominent location (Like model profile page) | The model name should be made clickable so that a user can be guided to the detail discount page | Offers to be shown on the basis of city which is already selected by user. In case of “no city” selection, default city (New Delhi) discounts to be shown | . Additional Pointers . User stories should not be written based on beliefs and ideas, but should be based on data and evidence. | User stories should be written collaboratively usually when the user story is being groomed. | A user story should not be confused with a task. A task represents “how”, a user story represents “why”. | .",
            "url": "http://kaushalbundel.page/user%20story/agile%20development/2021/01/19/user-story.html",
            "relUrl": "/user%20story/agile%20development/2021/01/19/user-story.html",
            "date": " • Jan 19, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Tutorial: Neural Style Transfer using Keras, Tensorflow",
            "content": "Objective: Build models for Neural style transfer to create interesting images and consequently learn about Neural Style Transfer. . Basics: Here we are going to compose one image in the style of another image. Neural style transfer is an optimization technique used to take two images—a content image and a style reference image (such as an artwork by a famous painter)—and blend them together so the output image looks like the content image, but “painted” in the style of the style reference image. This is implemented by optimizing the output image to match the content statistics of the content image and the style statistics of the style reference image. These statistics are extracted from the images using a convolutional network. . import os import tensorflow as tf # Load compressed models from Tensorflow hub os.environ[&#39;TFHUB_MODEL_LOAD_FORMAT&#39;]=&#39;COMPRESSED&#39; gpus= tf.config.experimental.list_physical_devices(&#39;GPU&#39;) #These above two lines are added while importing libraries as I am training the model on GPU and I am getting a CUDNN error while compiling the model. tf.config.experimental.set_memory_growth(gpus[0], True) #After searching for the solution I found that either I need to update my tensorflow or I need to add these two lines in the code. . import IPython.display as display #to display images import matplotlib.pyplot as plt import matplotlib as mpl mpl.rcParams[&#39;figure.figsize&#39;]=(12,12) # defining the figuresize mpl.rcParams[&#39;axes.grid&#39;]= False import numpy as np import PIL.Image #python image library import time import functools . content_path= tf.keras.utils.get_file(&#39;YellowLabradorLooking.jpg&#39;,&#39;https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg&#39;) style_path=tf.keras.utils.get_file(&#39;kandinsky_1.jpg&#39;,&#39;https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg&#39;) . tf.keras.utils.get_file downloads the respective images to local path and saves it there. . Visualizing the input . Defining a function to load an images and limit its dimension to 512 pixel (though that size can be changed later) . In the tutorial there are numerous terms which I am not aware about. Here in the below cell I will be decomposing some functions which I do not understand. This decomposition will help me to understand the function composition in a better manner. . max_dim=512 path_to_image=content_path img=tf.io.read_file(path_to_image) # Reading the image img=tf.image.decode_image(img, channels=3) # decoding the image into three different channels # tensor is having shape (577, 700,3) and the tensors are in uint8 format img = tf.image.convert_image_dtype(img, tf.float32) #Image is converted into float shape=tf.cast(tf.shape(img)[:-1], tf.float32) #The above command removes the channel (3) from the code long_dim=max(shape) scale=max_dim/long_dim new_shape=tf.cast(shape*scale, tf.int32) img=tf.image.resize(img, new_shape) img = img[tf.newaxis, :] # to add new dimension to a axis img.shape . TensorShape([1, 422, 512, 3]) . def load_img(path_to_img): max_dim = 512 img = tf.io.read_file(path_to_img) img = tf.image.decode_image(img, channels=3) img = tf.image.convert_image_dtype(img, tf.float32) shape = tf.cast(tf.shape(img)[:-1], tf.float32) long_dim = max(shape) scale = max_dim / long_dim new_shape = tf.cast(shape * scale, tf.int32) img = tf.image.resize(img, new_shape) img = img[tf.newaxis, :] return img . def imshow(image, title=None): if len(image.shape)&gt;3: image = tf.squeeze(image, axis=0) # tf.squeeze() function returns a tensor with the same value as its first argument, but a different shape. It removes dimensions whose size is one. plt.imshow(image) if title: plt.title(title) . content_image=load_img(content_path) style_image=load_img(style_path) plt.subplot(1,2,1) imshow(content_image,&#39;content_image&#39;) plt.subplot(1,2,2) imshow(style_image,&#39;style_image&#39;) . Fast Style Transfer using Transfer Hub . import tensorflow_hub as hub hub_model=hub.load(&#39;https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2&#39;) . The output of the above model is a tensor, so creating a function to generate images from tensor. . def tensor_to_image(tensor): tensor= tensor*255 tensor=np.array(tensor, dtype=np.uint8) if np.ndim(tensor)&gt;3: assert tensor.shape[0]==1 tensor=tensor[0] return PIL.Image.fromarray(tensor) . stylized_image= hub_model(tf.constant(content_image), tf.constant(style_image))[0] tensor_to_image(stylized_image) . Using manual mode for image modification . Use the intermediate layers of the model to get the content and style representations of the image. Starting from the network&#39;s input layer, the first few layer activations represent low-level features like edges and textures. As you step through the network, the final few layers represent higher-level features—object parts like wheels or eyes. In this case, you are using the VGG19 network architecture, a pretrained image classification network. These intermediate layers are necessary to define the representation of content and style from the images. . Loading a VGG model for doing the changes . x= tf.keras.applications.vgg19.preprocess_input(content_image*255) #preprossing the image to so that the image can be fed in the model x=tf.image.resize(x,(224,224)) # The input to the VGG model has to be resize as per the specification (224x 224) is the default size # Ref: https://keras.io/api/applications/vgg/#vgg19-function vgg= tf.keras.applications.VGG19(include_top=True, weights=&#39;imagenet&#39;) # loading the model, include_top: whether to include the 3 fully-connected layers at the top of the network. prediction_probablities=vgg(x) prediction_probablities.shape . TensorShape([1, 1000]) . predicted_top_5= tf.keras.applications.vgg19.decode_predictions(prediction_probablities.numpy())[0] predicted_top_5 . [(&#39;n02099712&#39;, &#39;Labrador_retriever&#39;, 0.49317113), (&#39;n02099601&#39;, &#39;golden_retriever&#39;, 0.23665294), (&#39;n02104029&#39;, &#39;kuvasz&#39;, 0.03635755), (&#39;n02099849&#39;, &#39;Chesapeake_Bay_retriever&#39;, 0.024182769), (&#39;n02107574&#39;, &#39;Greater_Swiss_Mountain_dog&#39;, 0.018646086)] . [(class_name, prob) for (number, class_name, prob) in predicted_top_5] . [(&#39;Labrador_retriever&#39;, 0.49317113), (&#39;golden_retriever&#39;, 0.23665294), (&#39;kuvasz&#39;, 0.03635755), (&#39;Chesapeake_Bay_retriever&#39;, 0.024182769), (&#39;Greater_Swiss_Mountain_dog&#39;, 0.018646086)] . This goes to show that the classifier is working fine. . Loading the VGG19 without the classification head. That means, we will not load the layers which are responsible for classification. . vgg= tf.keras.applications.VGG19(include_top=False, weights=&#39;imagenet&#39;) . for layer in vgg.layers: print(layer.name) . input_2 block1_conv1 block1_conv2 block1_pool block2_conv1 block2_conv2 block2_pool block3_conv1 block3_conv2 block3_conv3 block3_conv4 block3_pool block4_conv1 block4_conv2 block4_conv3 block4_conv4 block4_pool block5_conv1 block5_conv2 block5_conv3 block5_conv4 block5_pool . For the neural style learning we can choose intermediate layers from the model . content_layers=[&#39;block5_conv2&#39;] style_layers=[&#39;block1_conv1&#39;,&#39;block2_conv1&#39;, &#39;block3_conv1&#39;, &#39;block4_conv1&#39;, &#39;block5_conv1&#39;] num_content_layers=len(content_layers) num_style_layers=len(style_layers) . At a high level, in order for a network to perform image classification (which this network has been trained to do), it must understand the image. This requires taking the raw image as input pixels and building an internal representation that converts the raw image pixels into a complex understanding of the features present within the image. . This is also a reason why convolutional neural networks are able to generalize well: they’re able to capture the invariances and defining features within classes (e.g. cats vs. dogs) that are agnostic to background noise and other nuisances. Thus, somewhere between where the raw image is fed into the model and the output classification label, the model serves as a complex feature extractor. By accessing intermediate layers of the model, you&#39;re able to describe the content and style of input images. . Model Building . The networks in tf.keras.applications are designed so you can easily extract the intermediate layer values using the Keras functional API. . To define a model using the functional API, specify the inputs and outputs: . model = Model(inputs, outputs) . This following function builds a VGG19 model that returns a list of intermediate layer outputs: . def vgg_layers(layer_names): &quot;&quot;&quot;Created a vgg model that return a list of intermediate layer output values&quot;&quot;&quot; #loading the pretrained VGG19 model with image net weights vgg= tf.keras.applications.VGG19(include_top=False, weights=&quot;imagenet&quot;) vgg.trainable=False outputs=[vgg.get_layer(name).output for name in layer_names] # layer names are getting generated and are being stored in a list model= tf.keras.Model([vgg.input], outputs) #This gives a model with intermediate layer results as output return model . style_extractor= vgg_layers(style_layers) . style_outputs= style_extractor(style_image*255) #looking at the statistic of each layer for name, output in zip(style_layers, style_outputs): print(name) print(&quot; shape: &quot;, output.numpy().shape) print(&quot; min: &quot;, output.numpy().min()) print(&quot; max: &quot;, output.numpy().max()) print(&quot; mean: &quot;, output.numpy().mean()) print() . block1_conv1 shape: (1, 336, 512, 64) min: 0.0 max: 835.5256 mean: 33.97525 block2_conv1 shape: (1, 168, 256, 128) min: 0.0 max: 4625.8857 mean: 199.82687 block3_conv1 shape: (1, 84, 128, 256) min: 0.0 max: 8789.239 mean: 230.78099 block4_conv1 shape: (1, 42, 64, 512) min: 0.0 max: 21566.135 mean: 791.24005 block5_conv1 shape: (1, 21, 32, 512) min: 0.0 max: 3189.2542 mean: 59.179478 . Calculate Style using Gram Matrix . The content of an image is represented by the values of the intermediate feature maps. . It turns out, the style of an image can be described by the means and correlations across the different feature maps. Calculate a Gram matrix that includes this information by taking the outer product of the feature vector with itself at each location, and averaging that outer product over all locations. . Excellent Article on Gram Matrix also style transfer in general . Objective: The objective of the next code block is to extract the style from the style image . def gram_matrix(input_tensor): result = tf.linalg.einsum(&#39;bijc,bijd-&gt;bcd&#39;, input_tensor, input_tensor) input_shape = tf.shape(input_tensor) num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32) return result/(num_locations) . Extracting the style and content from any image . class StyleContentModel(tf.keras.models.Model): def __init__(self, style_layers, content_layers): super(StyleContentModel, self).__init__() self.vgg= vgg_layers(style_layers + content_layers) self.style_layers= style_layers self.content_layers= content_layers self.num_style_layers=len(style_layers) self.vgg.trainable= False def call(self, inputs): &quot;&quot;&quot;Expects input in float&quot;&quot;&quot; inputs=inputs*255 preprocessed_input= tf.keras.applications.vgg19.preprocess_input(inputs) outputs= self.vgg(preprocessed_input) style_outputs, content_outputs = (outputs[:self.num_style_layers], outputs[self.num_style_layers:]) style_outputs=[gram_matrix(style_output) for style_output in style_outputs] content_dict={content_name:value for content_name, value in zip(self.content_layers, content_outputs)} style_dict = {style_name:value for style_name, value in zip(self.style_layers, style_outputs)} return {&#39;content&#39;: content_dict, &#39;style&#39;: style_dict} . extractor= StyleContentModel(style_layers, content_layers) results = extractor(tf.constant(content_image)) # tf.constant is used to generated a tensor like object from a list. Here we have generated a tensor from content_image . results[&#39;style&#39;][&#39;block1_conv1&#39;] . &lt;tf.Tensor: shape=(1, 64, 64), dtype=float32, numpy= array([[[1.1075182e+03, 1.4356364e+02, 5.5064569e+02, ..., 2.5202802e+02, 3.3782355e+02, 6.8202942e+02], [1.4356364e+02, 3.9073691e+02, 5.0562070e+02, ..., 9.7891167e-02, 1.2800130e+02, 3.4219815e+02], [5.5064569e+02, 5.0562070e+02, 1.2310433e+03, ..., 5.6324894e+01, 4.7135349e+02, 9.5489502e+02], ..., [2.5202802e+02, 9.7891167e-02, 5.6324894e+01, ..., 7.8090027e+02, 1.6804555e+02, 1.3731578e+02], [3.3782355e+02, 1.2800130e+02, 4.7135349e+02, ..., 1.6804555e+02, 6.5903876e+02, 7.1504028e+02], [6.8202942e+02, 3.4219815e+02, 9.5489502e+02, ..., 1.3731578e+02, 7.1504028e+02, 1.1265114e+03]]], dtype=float32)&gt; . Run Gradient Descent . style_targets= extractor(style_image)[&#39;style&#39;] content_targets= extractor(content_image)[&#39;content&#39;] . Define a tf.Variable to contain the image to optimize. To make this quick, initialize it with the content image (the tf.Variable must be the same shape as the content image): . image = tf.Variable(content_image) . def clip_0_1(image): return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0) # clips a tensor values between a min and a max . opt= tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1) . style_weight=1e-2 content_weight=1e4 . def style_content_loss(outputs): style_outputs= outputs[&#39;style&#39;] content_outputs=outputs[&#39;content&#39;] style_loss= tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) for name in style_outputs.keys()]) style_loss*=style_weight / num_style_layers content_loss= tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) for name in content_outputs.keys()]) content_loss*=content_weight / num_content_layers loss= content_loss + style_loss return loss . #using tf.GradientTape to record the gradients while updating the image @tf.function() # Compiles a function into a callable TensorFlow graph. def train_step(image): with tf.GradientTape() as tape: outputs= extractor(image) loss= style_content_loss(outputs) grad=tape.gradient(loss, image) opt.apply_gradients([(grad, image)]) image.assign(clip_0_1(image)) . train_step(image) train_step(image) train_step(image) train_step(image) tensor_to_image(image) . import time start=time.time() epochs=1 steps_per_epoch=100 step=0 for n in range(epochs): for m in range(steps_per_epoch): step+=1 train_step(image) print(&quot;.&quot;, end=&#39;&#39;) display.clear_output(wait=True) display.display(tensor_to_image(image)) f&quot;Train step: {step}&quot; end=time.time() f&quot;Total time: {end-start}&quot; . &#39;Total time: 10.478002548217773&#39; . Total Variation loss . As per the google colab file the basic implementation produces lots of high quality artifacts. We can decreasse these uisng an explicit regularization term on the high frequency components of the image. . Detailed implementation is in the colab notebook. But for practical use tensorflow also provides a standard implementation which is tf.image.total_variation(image).numpy() . Re Running the implementation . total_variation_weight=30 #including the same in train_step function @tf.function() def train_step(image): with tf.GradientTape() as tape: outputs = extractor(image) loss = style_content_loss(outputs) loss += total_variation_weight*tf.image.total_variation(image) grad = tape.gradient(loss, image) opt.apply_gradients([(grad, image)]) image.assign(clip_0_1(image)) . image=tf.Variable(content_image) . import time start=time.time() epochs=1 steps_per_epoch=100 step=0 for n in range(epochs): for m in range(steps_per_epoch): step+=1 train_step(image) print(&quot;.&quot;, end=&#39;&#39;) display.clear_output(wait=True) display.display(tensor_to_image(image)) f&quot;Train step: {step}&quot; end=time.time() f&quot;Total time: {end-start}&quot; . &#39;Total time: 11.810032844543457&#39; . Reference and Next Steps . Reference Tutorial . Advance video style- to be explored . Excellent Resource: Neural Style Transfer . Reference: Usage of super in classes . Excellent Resource: Neural Style Transfer . Excellent Article on Neural Style Transfer . Artistic style cartoon implementation .",
            "url": "http://kaushalbundel.page/tensorflow/neural%20style%20transfer/images/keras/2020/12/08/Neural_Style_Transfer.html",
            "relUrl": "/tensorflow/neural%20style%20transfer/images/keras/2020/12/08/Neural_Style_Transfer.html",
            "date": " • Dec 8, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Datascraping using Twitter",
            "content": "My major objective of learning AI/ML is to understand the world around me from a data perspective. Twitter is a plateform which gives extensive natural language data, based on which lots of exploration can be done. In this post I am not going to do much exploration, but this article will focus on extracting data from twitter. The extraction is not exhaustive by any means and I will be updating this post at a later point in time as need for additional/new data arises. . #importing libraries import pandas as pd import tweepy import time import warnings warnings.filterwarnings(&#39;ignore&#39;) . . To access the Twitter API, we will need 4 things from the your Twitter App page. These keys are located in your Twitter app settings in the Keys and Access Tokens tab: . consumer key | consumer seceret key | access token key | access token secret key | . Detailed info for getting keys . #for privacy purposes, these keys are encrypted consumer_key=&quot;xxxxxxxxxxxxxxxxxxxxxxxxxx&quot; consumer_secret=&quot;xxxxxxxxxxxxxxxxxxxxxxxxxx&quot; access_token=&quot;xxxxxxxxxxxxxxxxxxxxxxxxxx&quot; access_token_secret=&quot;xxxxxxxxxxxxxxxxxxxxxxxxxx&quot; . . #Accessing twitter API auth=tweepy.OAuthHandler(consumer_key, consumer_secret) auth.set_access_token=(access_token, access_token_secret) api= tweepy.API(auth, wait_on_rate_limit= True) . . One major problem the is being faced is that when we convert a list in data frame, if the tweet text is &gt;50 chars it gets truncated. To solve this issue, we need to set display column width for pandas dataframe to be -1. . pd.set_option(&#39;display.max_colwidth&#39;, -1) . . Extracting the tweets of a specific user . # Doing a function implementation of the tweets # In this case if the tweet is retweeted we wont get full text def get_tweets(username, count): try: #creating query methods using parameters tweets= tweepy.Cursor(api.user_timeline,id= username, tweet_mode=&quot;extended&quot;).items(count) tweet_list= [[tweet.created_at, tweet.id, tweet.full_text] for tweet in tweets] #creating dataframe from tweets list tweets_df=pd.DataFrame(tweet_list, columns=[&quot;Date&quot;,&quot;Tweet_id&quot;,&quot;Tweet_Text&quot;]) except BaseException as e: print(&#39;failed on_status&#39;, str(e)) time.sleep(3) return tweets_df . . df=get_tweets(&quot;kunalb11&quot;,10) df . Date Tweet_id Tweet_Text . 0 2020-12-03 13:03:23 | 1334483368088518657 | @getpeid That’s not celebrating. That’s pausing. | . 1 2020-12-03 09:20:17 | 1334427226339594240 | Truly ambitious people don’t celebrate small milestones. | . 2 2020-12-02 18:26:36 | 1334202321685729282 | @simonsinek Also true for viruses. | . 3 2020-12-02 08:23:03 | 1334050432184852485 | @paraschopra @samueleonelia Make a post on making better todos. This is not very effective. | . 4 2020-12-02 05:34:29 | 1334008013133393920 | @Nithin0dha You’re smart with your money and stake. No wonder you didn’t hesitate to invest in CRED 😜 | . 5 2020-12-02 04:27:24 | 1333991129621139457 | You are your feedback loops. | . 6 2020-12-01 14:20:51 | 1333778089244037121 | There are no over ambitious people. Only dreamers with no plans. | . 7 2020-12-01 14:20:09 | 1333777913733402627 | @warikoo https://t.co/kzWnlQ26Na | . 8 2020-12-01 14:10:16 | 1333775427672838144 | @thepavantej @ShaswatShetty They have a plan. | . 9 2020-12-01 13:57:37 | 1333772241025134597 | @ShaswatShetty Yes. If they lack maturity. | . Extracting tweets from a perticular string . text_query=&quot;IPL -filter:retweets&quot; count=10 try: #creation of query method using parameters tweets= tweepy.Cursor(api.search, q=text_query, tweet_mode=&quot;extended&quot;, lang=&#39;en&#39;).items(count) #getting the information from twitter object tweet_list= [[tweet.created_at, tweet.id, tweet.full_text] for tweet in tweets] #creating a data frame from the list tweet_df_from_query= pd.DataFrame(tweet_list, columns=[&#39;Date&#39;,&#39;tweet_id&#39;, &#39;tweet_text&#39;]) except BaseException as e: print(&#39;failed_on_status&#39;, str(e)) time.sleep(3) . tweet_df_from_query . Date tweet_id tweet_text . 0 2020-12-03 14:50:07 | 1334510230957395969 | @man4_cricket You change your comment you say after ipl rohit should lead one day and t 20 | . 1 2020-12-03 14:49:25 | 1334510052661719045 | @ItsYashswiniR Will you support that gujarat team if it comes to ipl or rcb | . 2 2020-12-03 14:47:33 | 1334509584283803651 | @T20Nomad I&#39;ve an idea. See how many top Indian players in IPL in a rolling 3 year window were selected in the then T20i team. Because I&#39;m betting it&#39;ll be pretty low. | . 3 2020-12-03 14:47:19 | 1334509527262261248 | @ankur9811 @faizanlakhani Our PSL is far superior in terms of quality, just check inaugural addition of IPL and check our players performance... | . 4 2020-12-03 14:47:16 | 1334509514821951491 | @T20Nomad So that&#39;s not IPL fault. Its selectors and team managements fault. We arent backing talents for long and those who has better abilities. | . 5 2020-12-03 14:45:44 | 1334509126093840384 | @RCBTweets I think after his performance in IPL 2020, you will not publish any posts about Aaron Finch!! ... Please Don&#39;t post anything About this fraud batsman.... He is a selfish!! ... Not interested in his related post 🙄🙄 | . 6 2020-12-03 14:43:17 | 1334508512215506945 | @AstikRaj10 @Pateljayraj12 @i_asli_rohan @Srihari94027224 @Pranav14445113 @Vikram38992074 @imVkohli @ImRo45 @IPL @davidwarner31 @stevesmith49 @CricketAus @Jaspritbumrah93 @msdhoni @BCCI If I&#39;m truly want Indian team as champion then y I opposed Kohli Captaincy | . 7 2020-12-03 14:43:17 | 1334508510181265409 | @NithinWatto_185 IPL time lo rts effect emo bro | . 8 2020-12-03 14:43:10 | 1334508481920036864 | @ESPNcricinfo I want Kerala back in IPL 2021. New logo of Kerala in IPL. I don&#39;t want old Kochi logo. | . 9 2020-12-03 14:43:01 | 1334508445253259264 | Aaron Finch in T20I 👑 vs IPL 🤡 n nVisible confusion 😔👍 https://t.co/q7lGNHXZGM | . . Additional Usage of the tweepy API . Documentation of tweepy API (http://docs.tweepy.org/en/latest/api.html#status-methods) . Extracting trends of a perticular region . trend=pd.DataFrame(api.trends_available()) str(trend[&#39;country&#39;]) trend[&#39;country&#39;]=&quot;India&quot; . [i[&#39;name&#39;] for i in api.trends_place(id=23424848)[0][&#39;trends&#39;]] . [&#39;#28YearsOfBelovedVIJAY&#39;, &#39;#Abyss&#39;, &#39;#DiljitDosanjh&#39;, &#39;#सब_पर_भारी_कंगना&#39;, &#39;#कंगना_चुपचाप_माफी_माँग&#39;, &#39;SIDHARTH SHUKLA X BBB3&#39;, &#39;Punjabi&#39;, &#39;PagarBook Lagao BusinessBadhao&#39;, &#39;Agastya&#39;, &#39;G.O.A.T&#39;, &#39;Karan Johar&#39;, &#39;Paaji&#39;, &#39;SHERNI RUBINA&#39;, &#39;marques&#39;, &#39;Hrithik Roshan&#39;, &#39;Diljeet&#39;, &#39;HIS VOICE&#39;, &#39;Jin oppa&#39;, &#39;Translate&#39;, &#39;Jhansi&#39;, &#39;Kollywood&#39;, &#39;Translation&#39;, &#39;Padma Vibhushan&#39;, &#39;BE ON TOP&#39;, &#39;Patanjali&#39;, &#39;#Master&#39;, &#39;#FarmersProtestDelhi2020&#39;, &#39;#HappyBirthdayJin&#39;, &#39;#BrokenButBeautiful&#39;, &#39;#MaplestoryForJin&#39;, &#39;#THANKYOUJIN&#39;, &#39;#ThursdayThoughts&#39;, &#39;#ThalapathyVijay&#39;, &#39;#BTSJIN&#39;, &#39;#தமிழர்_நாட்டை_தமிழர்_ஆள்வோம்&#39;, &#39;#CBIRevealSSRMystery&#39;, &#39;#JiahKhanAwaitsJustice&#39;, &#39;#MiSpotify2020&#39;, &#39;#WonderWoman1984&#39;, &#39;#இப்போ_இல்லேன்னா_எப்பவும்_இல்ல&#39;, &#39;#कंगना_रानौत_शेरनी_है&#39;, &#39;#SaHad&#39;, &#39;#OurSparklingGemJin&#39;, &#39;#rajinikanthpolticalentry&#39;, &#39;#rubam&#39;, &#39;#BrightestDiamondJin&#39;, &#39;#SoniaRathee&#39;, &#39;#atkmbofc&#39;, &#39;#CBIFile302InJiahKhanCase&#39;, &#39;#PunyaPaap&#39;] . Next steps . There can be lots of things which can be done using data scrapped from twitter. Some of the uses can be: . Sentiment analysis can be done on specific twitter trends/Users | Geographic mapping of different twitter trends can be done | Analysis of a user&#39;s tweet can be done (word cloud) to find out which things intrest that perticular user | Tweet bot can be created, which can post regular tweets on your behalf | . and many more... . Let&#39;s hope to explore many of these details in the future . . References . [1] http://docs.tweepy.org/en/latest/api.html#status-methods [2] https://www.earthdatascience.org/courses/use-data-open-source-python/intro-to-apis/twitter-data-in-python/ [3] https://towardsdatascience.com/how-to-scrape-tweets-from-twitter-59287e20f0f1 .",
            "url": "http://kaushalbundel.page/twitter/nlp/pandas/data%20scraping/2020/12/03/Twitter_Data_Scrapping.html",
            "relUrl": "/twitter/nlp/pandas/data%20scraping/2020/12/03/Twitter_Data_Scrapping.html",
            "date": " • Dec 3, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Practical Project in Deep Learning: Car Sales Prediction",
            "content": "This is one of the first deep learning models created by me. Here I am trying to do regression on a real world data, and I am trying to predict a expected sale amount of a used car. . Project Overview . You are working as a car salesman and you would like to develop a model to predict the total dollar amount that customers are willing to pay given the following attributes: . Customer Name | Customer e-mail | Country | Gender | Age | Annual Salary | Credit Card Debt | Net Worth | . The model should predict: Car Purchase Amount . Solution . The solution is derived from a simple Deep learning model using Keras- Tensorflow. . #importing libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import tensorflow as tf import os import seaborn as sns . . os.getcwd() . . &#39;C: Users kaush Desktop Machine Learning Udemy&#39; . df=pd.read_csv(&#39;C: Users kaush Desktop Machine Learning Udemy 8 REal world projects original DL and ML Practical Tutorials - Package Project 1 Car_Purchasing_Data.csv&#39;, encoding=&#39;mac-roman&#39;) . . df.info() . . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 500 entries, 0 to 499 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 Customer Name 500 non-null object 1 Customer e-mail 500 non-null object 2 Country 500 non-null object 3 Gender 500 non-null int64 4 Age 500 non-null float64 5 Annual Salary 500 non-null float64 6 Credit Card Debt 500 non-null float64 7 Net Worth 500 non-null float64 8 Car Purchase Amount 500 non-null float64 dtypes: float64(5), int64(1), object(3) memory usage: 35.3+ KB . There are no null values present in the dataset. . sns.pairplot(df) . . &lt;seaborn.axisgrid.PairGrid at 0x1dd7dcd88c8&gt; . The data is also normally distributed. . Data PreProcessing . Dividing the data into dependent and independent variables . X=df[[&#39;Gender&#39;, &#39;Age&#39;,&#39;Annual Salary&#39;, &#39;Credit Card Debt&#39;, &#39;Net Worth&#39;]] . . df.columns . . Index([&#39;Customer Name&#39;, &#39;Customer e-mail&#39;, &#39;Country&#39;, &#39;Gender&#39;, &#39;Age&#39;, &#39;Annual Salary&#39;, &#39;Credit Card Debt&#39;, &#39;Net Worth&#39;, &#39;Car Purchase Amount&#39;], dtype=&#39;object&#39;) . Y=df[[&#39;Car Purchase Amount&#39;]] . . #Shape of these two datasets X.shape, Y.shape . . ((500, 5), (500, 1)) . Data Transformation . from sklearn.preprocessing import MinMaxScaler scaler=MinMaxScaler() X_scaled=scaler.fit_transform(X) . . y_scaled=scaler.fit_transform(Y) . . y_scaled.shape . . (500, 1) . Train and Test set Division . #Dividing the dataset Training and test set from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test= train_test_split(X_scaled, y_scaled, test_size=0.2) . . #Getting the shape of train and test set X_train.shape, X_test.shape,y_train.shape, y_test.shape . . ((400, 5), (100, 5), (400, 1), (100, 1)) . Model Creation . #Building a model (Simple view) # Importing model busilding libarary model=tf.keras.Sequential() model.add(tf.keras.layers.Dense(25,input_dim=5,activation=&#39;relu&#39;)) model.add(tf.keras.layers.Dense(25,activation=&#39;relu&#39;)) model.add(tf.keras.layers.Dense(1,activation=&#39;linear&#39;)) . . model.summary() . . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 25) 150 _________________________________________________________________ dense_1 (Dense) (None, 25) 650 _________________________________________________________________ dense_2 (Dense) (None, 1) 26 ================================================================= Total params: 826 Trainable params: 826 Non-trainable params: 0 _________________________________________________________________ . Model Compilation . # Compile and fit the model the model model.compile(optimizer=&#39;adam&#39;, loss=tf.keras.losses.MeanSquaredError()) epoch_hist=model.fit(X_train, y_train, batch_size=32,epochs=50, verbose=1, validation_split=0.2 ) . . Train on 320 samples, validate on 80 samples Epoch 1/50 320/320 [==============================] - 1s 2ms/sample - loss: 0.0268 - val_loss: 0.0108 Epoch 2/50 320/320 [==============================] - 0s 94us/sample - loss: 0.0114 - val_loss: 0.0078 Epoch 3/50 320/320 [==============================] - 0s 97us/sample - loss: 0.0079 - val_loss: 0.0083 Epoch 4/50 320/320 [==============================] - 0s 100us/sample - loss: 0.0068 - val_loss: 0.0062 Epoch 5/50 320/320 [==============================] - 0s 87us/sample - loss: 0.0057 - val_loss: 0.0050 Epoch 6/50 320/320 [==============================] - 0s 90us/sample - loss: 0.0048 - val_loss: 0.0044 Epoch 7/50 320/320 [==============================] - 0s 93us/sample - loss: 0.0042 - val_loss: 0.0035 Epoch 8/50 320/320 [==============================] - 0s 94us/sample - loss: 0.0035 - val_loss: 0.0032 Epoch 9/50 320/320 [==============================] - 0s 87us/sample - loss: 0.0030 - val_loss: 0.0026 Epoch 10/50 320/320 [==============================] - 0s 69us/sample - loss: 0.0025 - val_loss: 0.0023 Epoch 11/50 320/320 [==============================] - 0s 78us/sample - loss: 0.0021 - val_loss: 0.0020 Epoch 12/50 320/320 [==============================] - 0s 78us/sample - loss: 0.0018 - val_loss: 0.0016 Epoch 13/50 320/320 [==============================] - 0s 78us/sample - loss: 0.0015 - val_loss: 0.0014 Epoch 14/50 320/320 [==============================] - 0s 75us/sample - loss: 0.0013 - val_loss: 0.0012 Epoch 15/50 320/320 [==============================] - 0s 78us/sample - loss: 0.0011 - val_loss: 0.0010 Epoch 16/50 320/320 [==============================] - 0s 72us/sample - loss: 0.0010 - val_loss: 8.8114e-04 Epoch 17/50 320/320 [==============================] - 0s 87us/sample - loss: 8.9070e-04 - val_loss: 8.9388e-04 Epoch 18/50 320/320 [==============================] - 0s 90us/sample - loss: 8.2632e-04 - val_loss: 6.9277e-04 Epoch 19/50 320/320 [==============================] - 0s 81us/sample - loss: 7.2381e-04 - val_loss: 5.9430e-04 Epoch 20/50 320/320 [==============================] - 0s 90us/sample - loss: 6.1710e-04 - val_loss: 5.3789e-04 Epoch 21/50 320/320 [==============================] - 0s 97us/sample - loss: 5.6844e-04 - val_loss: 4.8474e-04 Epoch 22/50 320/320 [==============================] - 0s 97us/sample - loss: 5.0552e-04 - val_loss: 4.6217e-04 Epoch 23/50 320/320 [==============================] - 0s 78us/sample - loss: 4.4945e-04 - val_loss: 4.3341e-04 Epoch 24/50 320/320 [==============================] - 0s 90us/sample - loss: 4.0751e-04 - val_loss: 3.4198e-04 Epoch 25/50 320/320 [==============================] - 0s 87us/sample - loss: 3.7346e-04 - val_loss: 3.0249e-04 Epoch 26/50 320/320 [==============================] - 0s 103us/sample - loss: 3.3161e-04 - val_loss: 2.6853e-04 Epoch 27/50 320/320 [==============================] - 0s 93us/sample - loss: 3.0061e-04 - val_loss: 2.2708e-04 Epoch 28/50 320/320 [==============================] - 0s 100us/sample - loss: 2.6968e-04 - val_loss: 2.1430e-04 Epoch 29/50 320/320 [==============================] - 0s 87us/sample - loss: 2.7415e-04 - val_loss: 2.0908e-04 Epoch 30/50 320/320 [==============================] - 0s 75us/sample - loss: 2.3847e-04 - val_loss: 2.0659e-04 Epoch 31/50 320/320 [==============================] - 0s 87us/sample - loss: 2.1483e-04 - val_loss: 2.2208e-04 Epoch 32/50 320/320 [==============================] - 0s 87us/sample - loss: 2.0754e-04 - val_loss: 1.6797e-04 Epoch 33/50 320/320 [==============================] - 0s 75us/sample - loss: 2.0345e-04 - val_loss: 1.1963e-04 Epoch 34/50 320/320 [==============================] - 0s 81us/sample - loss: 1.7502e-04 - val_loss: 1.0149e-04 Epoch 35/50 320/320 [==============================] - 0s 75us/sample - loss: 1.4977e-04 - val_loss: 9.9708e-05 Epoch 36/50 320/320 [==============================] - 0s 90us/sample - loss: 1.3988e-04 - val_loss: 7.8225e-05 Epoch 37/50 320/320 [==============================] - 0s 81us/sample - loss: 1.2686e-04 - val_loss: 7.5219e-05 Epoch 38/50 320/320 [==============================] - 0s 72us/sample - loss: 1.1346e-04 - val_loss: 6.5291e-05 Epoch 39/50 320/320 [==============================] - 0s 81us/sample - loss: 1.0530e-04 - val_loss: 6.0129e-05 Epoch 40/50 320/320 [==============================] - 0s 78us/sample - loss: 9.6780e-05 - val_loss: 5.9240e-05 Epoch 41/50 320/320 [==============================] - 0s 87us/sample - loss: 9.5623e-05 - val_loss: 5.3118e-05 Epoch 42/50 320/320 [==============================] - 0s 87us/sample - loss: 8.2329e-05 - val_loss: 4.7496e-05 Epoch 43/50 320/320 [==============================] - 0s 75us/sample - loss: 7.4392e-05 - val_loss: 4.4520e-05 Epoch 44/50 320/320 [==============================] - 0s 84us/sample - loss: 6.7060e-05 - val_loss: 4.3026e-05 Epoch 45/50 320/320 [==============================] - 0s 75us/sample - loss: 6.4274e-05 - val_loss: 3.7107e-05 Epoch 46/50 320/320 [==============================] - 0s 90us/sample - loss: 6.4040e-05 - val_loss: 4.6213e-05 Epoch 47/50 320/320 [==============================] - 0s 87us/sample - loss: 5.7554e-05 - val_loss: 3.8434e-05 Epoch 48/50 320/320 [==============================] - 0s 75us/sample - loss: 5.7327e-05 - val_loss: 3.2283e-05 Epoch 49/50 320/320 [==============================] - 0s 94us/sample - loss: 5.4043e-05 - val_loss: 3.6133e-05 Epoch 50/50 320/320 [==============================] - 0s 97us/sample - loss: 5.2156e-05 - val_loss: 3.0301e-05 . # Evaluating the data epoch_hist.history.keys() . . dict_keys([&#39;loss&#39;, &#39;val_loss&#39;]) . Model Performance . #Plotting a graph of model performance plt.plot(epoch_hist.history[&#39;loss&#39;]) plt.plot(epoch_hist.history[&#39;val_loss&#39;]) plt.title(&quot;Linear Model- Loss vs Epochs&quot;) plt.xlabel(&quot;No of epochs&quot;) plt.ylabel(&quot;Loss&quot;) plt.legend([&#39;Loss&#39;,&#39;Validation Loss&#39;]) plt.show() . . Prediction using model and error Calculation . #predicting the test results y_pred=model.predict(X_test) . . # Calculating the error between Y pred and Y test from sklearn.metrics import mean_squared_error print(&quot;The error between test data and predicted numbers is:&quot;, mean_squared_error(y_test, y_pred)) . . The error between test data and predicted numbers is: 3.8762125725253964e-05 . Conclusion and Next Steps . This is a very simple example of DNN implementation. In the next steps of this case, we can even create an API, which can have input parameters as the parameters of the model so that the model could be utilized in production. .",
            "url": "http://kaushalbundel.page/deep%20learning/tensorflow/keras/real%20life%20study/2020/12/03/Car_Sales_prediction_using_Simple_DeepNeuralNetwork.html",
            "relUrl": "/deep%20learning/tensorflow/keras/real%20life%20study/2020/12/03/Car_Sales_prediction_using_Simple_DeepNeuralNetwork.html",
            "date": " • Dec 3, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Understanding the timeseries aspect on the air quality of Delhi",
            "content": "It is stated very normally that the air quality is pretty worse during/after diwali period. In this data exercise I would be experimenting with the time series aspect of the air quality using facebook&#39;s api &quot;Prophet&quot;. I would like to understand how weekends, festival season and general seasonality impact the air quality index for Delhi city . #importing liberaries import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import os import warnings from fbprophet import Prophet import statsmodels.api as sm warnings.filterwarnings(&quot;ignore&quot;) %matplotlib inline . . #reading the datase df=pd.read_csv(&#39;city_day.csv&#39;) #extracting the data for date and AQI dfd=df[df[&#39;City&#39;]==&#39;Delhi&#39;][[&#39;Date&#39;,&#39;AQI&#39;]].reset_index() dfd=dfd.drop(labels=&#39;index&#39;, axis=1) . . #inspecting the extracted dataset #this is the entire city data for Delhi dfd.info() . . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2009 entries, 0 to 2008 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 Date 2009 non-null object 1 AQI 1999 non-null float64 dtypes: float64(1), object(1) memory usage: 31.5+ KB . This shows that the data is comprised of two coloumns one Date and the other is AQI. We need to convert the Date which is an object into Datetime object. . #converting the same to data object dfd[&#39;Date&#39;]=pd.to_datetime(df[&#39;Date&#39;]) . . dfd.info() #As can be seen here that the date time is converted from object to Datetime . . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2009 entries, 0 to 2008 Data columns (total 2 columns): # Column Non-Null Count Dtype -- -- 0 Date 2009 non-null datetime64[ns] 1 AQI 1999 non-null float64 dtypes: datetime64[ns](1), float64(1) memory usage: 31.5 KB . #plotting the time series data using matplotlib dfd.plot(x=&#39;Date&#39;, y=&#39;AQI&#39;,kind=&#39;line&#39;, title=&quot;Primary Graph-AQI vs year&quot;, grid= True, figsize=(20, 5)) . . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f123423baf0&gt; . #Checking for missing datas dfd.isna().sum() . . Date 0 AQI 10 dtype: int64 . As we can see we have 10 missing values in the dataset . #imputing the missing values #Checking which fields have missingg values dfd[dfd[&#39;AQI&#39;].isna()] . . Date AQI . 570 2016-07-24 | NaN | . 904 2017-06-23 | NaN | . 955 2017-08-13 | NaN | . 956 2017-08-14 | NaN | . 964 2017-08-22 | NaN | . 965 2017-08-23 | NaN | . 968 2017-08-26 | NaN | . 969 2017-08-27 | NaN | . 970 2017-08-28 | NaN | . 971 2017-08-29 | NaN | . Here, we can see that majorily the data is abscent from 22nd to 29th Aug for 2017. Correcting the same. . dfd[&#39;AQI&#39;]=dfd[&#39;AQI&#39;].fillna(dfd[&#39;AQI&#39;].rolling(4, min_periods=1).mean()) . . dfd[&#39;AQI&#39;].isna().sum() . . 0 . Exploratory Data Analysis . We will first see the trend of the data with respect to different date range. . #creating a function to extract different date parameters def date_feature(df, label=None): df= df.copy() df[&#39;date&#39;]= df[&#39;Date&#39;] df[&#39;month&#39;]=df[&#39;Date&#39;].dt.strftime(&#39;%B&#39;) df[&#39;year&#39;]=df[&#39;Date&#39;].dt.strftime(&#39;%Y&#39;) df[&#39;dayofweek&#39;]=df[&#39;Date&#39;].dt.strftime(&#39;%A&#39;) df[&#39;quarter&#39;]=df[&#39;Date&#39;].dt.quarter df[&#39;dayofyear&#39;]=df[&#39;Date&#39;].dt.dayofyear df[&#39;dayofmonth&#39;]=df[&#39;Date&#39;].dt.day df[&#39;weekofyear&#39;]=df[&#39;Date&#39;].dt.weekofyear X= df[[&#39;date&#39;,&#39;month&#39;,&#39;year&#39;,&#39;dayofweek&#39;,&#39;quarter&#39;,&#39;dayofyear&#39;,&#39;dayofmonth&#39;,&#39;weekofyear&#39;]] if label: y= df[label] return X, y return X X, y= date_feature(dfd, label=&#39;AQI&#39;) . . df_new=pd.concat([X,y], axis=1) df_new.head() . . date month year dayofweek quarter dayofyear dayofmonth weekofyear AQI . 0 2015-01-01 | January | 2015 | Thursday | 1 | 1 | 1 | 1 | 472.0 | . 1 2015-01-02 | January | 2015 | Friday | 1 | 2 | 2 | 1 | 454.0 | . 2 2015-01-03 | January | 2015 | Saturday | 1 | 3 | 3 | 1 | 143.0 | . 3 2015-01-04 | January | 2015 | Sunday | 1 | 4 | 4 | 1 | 319.0 | . 4 2015-01-05 | January | 2015 | Monday | 1 | 5 | 5 | 2 | 325.0 | . # Plotting different features to see differnet trends plt.figure(figsize=(12,5)) pallette= sns.color_palette(&quot;mako_r&quot;, 4) a=sns.barplot(data=df_new, x=&#39;month&#39;, y=&#39;AQI&#39;, hue=&#39;year&#39;) a.set_title(&quot;Month wise trend for AQI&quot;) plt.legend(loc=&#39;best&#39;) plt.show() . . We can see from this graph that the AQI is best in the summser season and goes to worst in winter season (esp bad in the month of Nov. due to Parali (Stubble buring and Diwali) . plt.figure(figsize=(12,5)) pallette= sns.color_palette(&quot;mako_r&quot;, 4) df_agg=df_new.groupby(&#39;year&#39;)[&#39;AQI&#39;].mean().reset_index() a=sns.barplot(data=df_agg, x=&#39;year&#39;, y=&#39;AQI&#39;, hue=&#39;year&#39;) a.set_title(&quot;year wise trend for AQI&quot;) plt.legend(loc=&#39;best&#39;) plt.show() . . In general the air quality is getting better for Delhi. . plt.figure(figsize=(12,5)) pallette= sns.color_palette(&quot;mako_r&quot;, 4) df_agg=df_new.groupby(&#39;month&#39;)[&#39;AQI&#39;].mean().sort_values(ascending=False).reset_index() a=sns.barplot(data=df_agg, x=&#39;month&#39;, y=&#39;AQI&#39;) a.set_title(&quot;month wise trend for AQI&quot;) #plt.legend(loc=&#39;best&#39;) plt.show() . . As it can be seen from the graph above that the worst air quality is in the month of November and the best air quality is in the month of July, August. This could be also due to the fact that genrally in these months monsoon arrives in Delhi resulting in better AQI vis a vis remaining months. . plt.figure(figsize=(12,5)) pallette= sns.color_palette(&quot;mako_r&quot;, 4) df_agg=df_new.groupby(&#39;dayofweek&#39;)[&#39;AQI&#39;].mean().sort_values(ascending=False).reset_index() a=sns.barplot(data=df_agg, x=&#39;dayofweek&#39;, y=&#39;AQI&#39;) a.set_title(&quot;Day of week wise trend for AQI&quot;) #plt.legend(loc=&#39;best&#39;) plt.show() . . Not much of a difference here. AQI is marginally better on Monday/Tuesday and Worst on Friday. . plt.figure(figsize=(12,5)) pallette= sns.color_palette(&quot;mako_r&quot;, 4) df_agg=df_new.groupby(&#39;quarter&#39;)[&#39;AQI&#39;].median().sort_values(ascending=False).reset_index() a=sns.barplot(data=df_agg, x=&#39;quarter&#39;, y=&#39;AQI&#39;) a.set_title(&quot;month wise trend for AQI&quot;) #plt.legend(loc=&#39;best&#39;) plt.show() . . This further reiterates the point that the fourth quarter has most severe values for AQI . plt.figure(figsize=(12,5)) pallette= sns.color_palette(&quot;mako_r&quot;, 4) df_agg=df_new.groupby(&#39;dayofmonth&#39;)[&#39;AQI&#39;].median().sort_values(ascending=False).reset_index() a=sns.barplot(data=df_agg, x=&#39;dayofmonth&#39;, y=&#39;AQI&#39;) a.set_title(&quot;days of month wise trend for AQI&quot;) #plt.legend(loc=&#39;best&#39;) plt.show() . . No perticular trend visible here. . plt.figure(figsize=(12,5)) pallette= sns.color_palette(&quot;mako_r&quot;, 4) df_agg=df_new.groupby(&#39;weekofyear&#39;)[&#39;AQI&#39;].median().sort_values(ascending=False).reset_index() a=sns.barplot(data=df_agg, x=&#39;weekofyear&#39;, y=&#39;AQI&#39;) a.set_title(&quot;week of the year wise trend for AQI&quot;) #plt.legend(loc=&#39;best&#39;) plt.show() . . The trend follows closely with the month trend . . Modeling the data usin fb prophet . We would be using the dfd dataframe. Also, dividing the data into training and test set. . f&quot;the total number of days considered are:{dfd[&#39;Date&#39;].count()}&quot; . . &#39;the total number of days considered are:2009&#39; . As of now we are doing univariate analysis. Also, prophet expects the date to be specified as &quot;ds&quot; and output to be specified as &quot;y&quot; . #remaining the column name dfd.columns=[&#39;ds&#39;,&#39;y&#39;] dfd.columns . . Index([&#39;ds&#39;, &#39;y&#39;], dtype=&#39;object&#39;) . #using 70% of the data as test data loc1=int(0.7*2009) dfd.loc[loc1] mask1=dfd[&#39;ds&#39;]&lt;=&#39;2018-11-07&#39; mask2=dfd[&#39;ds&#39;]&gt;&#39;2018-11-07&#39; X_train=dfd.loc[mask1] X_test=dfd.loc[mask2] f&quot;the training size is: {X_train.shape}&quot;,f&quot;the test size is: {X_test.shape}&quot; . . (&#39;the training size is: (1407, 2)&#39;, &#39;the test size is: (602, 2)&#39;) . #plotiing the train and test data in a single plot fig,ax= plt.subplots(figsize=(12,5)) X_train.plot(kind=&#39;line&#39;, x=&#39;ds&#39;, y=&#39;y&#39;, color=&#39;green&#39;,ax=ax) X_test.plot(kind=&#39;line&#39;, x=&#39;ds&#39;, y=&#39;y&#39;, color=&#39;red&#39;,ax=ax) plt.title(&quot;Train vs Test distribution&quot;) plt.grid(True) . . Around 70% of the data is being used for training and remaining 30% is being used as test data . Now we need a function to determine how far the prediction is from the actual value . #creating the error function def mean_absolute_per_error(y_true,y_pred): y_true, y_pred= np.array(y_true), np.array(y_pred) return np.mean(np.abs(((y_true-y_pred)/y_true)*100)) . . Creating a simple prophet model . model=Prophet() model.fit(X_train) . . INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. . &lt;fbprophet.forecaster.Prophet at 0x7f12342b3580&gt; . #doing a simple prediction future=model.make_future_dataframe(periods=602, freq=&#39;D&#39;) forecast=model.predict(future) . . forecast[[&#39;ds&#39;,&#39;yhat&#39;,&#39;yhat_lower&#39;,&#39;yhat_upper&#39;]].tail() . . ds yhat yhat_lower yhat_upper . 2004 2020-06-27 | 139.226799 | 42.789036 | 235.948905 | . 2005 2020-06-28 | 123.716330 | 32.619498 | 213.518386 | . 2006 2020-06-29 | 113.079597 | 18.350319 | 205.135138 | . 2007 2020-06-30 | 113.825239 | 19.581113 | 212.675103 | . 2008 2020-07-01 | 115.170619 | 15.735545 | 205.362100 | . The values as predicted are mentioned here. now comparing these values with the actual values . X_train.tail() . . ds y . 1402 2018-11-03 | 353.0 | . 1403 2018-11-04 | 238.0 | . 1404 2018-11-05 | 424.0 | . 1405 2018-11-06 | 411.0 | . 1406 2018-11-07 | 317.0 | . As we can see there is a decent difference in prediction numbers. . #plotting the components of the model fig=model.plot_components(forecast) . . Comparing actual Data with forecast . X_test_forecast=model.predict(X_test) X_test_forecast.tail() . . ds trend yhat_lower yhat_upper trend_lower trend_upper additive_terms additive_terms_lower additive_terms_upper weekly weekly_lower weekly_upper yearly yearly_lower yearly_upper multiplicative_terms multiplicative_terms_lower multiplicative_terms_upper yhat . 597 2020-06-27 | 233.905903 | 39.348156 | 227.112222 | 192.888383 | 271.729208 | -94.679104 | -94.679104 | -94.679104 | 6.987876 | 6.987876 | 6.987876 | -101.666980 | -101.666980 | -101.666980 | 0.0 | 0.0 | 0.0 | 139.226799 | . 598 2020-06-28 | 233.897329 | 33.187360 | 219.898673 | 192.735012 | 271.804260 | -110.180999 | -110.180999 | -110.180999 | -4.077221 | -4.077221 | -4.077221 | -106.103777 | -106.103777 | -106.103777 | 0.0 | 0.0 | 0.0 | 123.716330 | . 599 2020-06-29 | 233.888756 | 16.514941 | 205.538715 | 192.581640 | 271.823890 | -120.809159 | -120.809159 | -120.809159 | -10.450574 | -10.450574 | -10.450574 | -110.358584 | -110.358584 | -110.358584 | 0.0 | 0.0 | 0.0 | 113.079597 | . 600 2020-06-30 | 233.880182 | 23.312637 | 207.195651 | 192.428269 | 271.841987 | -120.054943 | -120.054943 | -120.054943 | -5.658074 | -5.658074 | -5.658074 | -114.396869 | -114.396869 | -114.396869 | 0.0 | 0.0 | 0.0 | 113.825239 | . 601 2020-07-01 | 233.871609 | 21.076836 | 210.825516 | 192.274898 | 271.850495 | -118.700990 | -118.700990 | -118.700990 | -0.513920 | -0.513920 | -0.513920 | -118.187070 | -118.187070 | -118.187070 | 0.0 | 0.0 | 0.0 | 115.170619 | . #comparing the test AQI with forecast AQI f,ax= plt.subplots(figsize=(12,5)) X_test.plot(kind=&#39;line&#39;, x=&#39;ds&#39;, y=&#39;y&#39;, color=&#39;blue&#39;, ax=ax, label=&quot;actual&quot;) X_test_forecast.plot(kind=&#39;line&#39;, x=&#39;ds&#39;, y=&#39;yhat&#39;, color=&#39;red&#39;, ax=ax, label=&quot;forecast&quot;) plt.title(&quot;Actual vs prediction (AQI)&quot;) plt.grid(True) plt.show() . . With this graph we can see that the pred vs actual is some what in the trend, but the range is not being captured ie. the peaks and troughs. . #calculating the error f&quot;The MAPE (Mean absolute Percentage error) for simple Prophet model is: {mean_absolute_per_error(X_test[&#39;y&#39;], X_test_forecast[&#39;yhat&#39;])}&quot; . . &#39;The MAPE (Mean absolute Percentage error) for simple Prophet model is: 38.77861304414898&#39; . The error % is quite large. We can now optimize the same using hyperparamter tuning. . Different hyperparameters which can be used in the model to optmize the model performance. . Holidays We can add holidays in the model using holiday liberary. Certain custom holidays can also be added. | n_changepoints is the number of change happen in the data. Prophet model detects them by its own. By default, its value is 25, which are uniformly placed in the first 80% of the time series. Changing n_changepoints can add value to the model. . | changepoint_prior_scale to indicate how flexible the changepoints are allowed to be. In other words, how much can the changepoints fit to the data. If you make it high it will be more flexible, but you can end up overfitting. By default, this parameter is set to 0.05 . | seasonality_mode There are 2 types model seasonality mode. Additive &amp; multiplicaticative. By default Prophet fits additive seasonalities, meaning the effect of the seasonality is added to the trend to get the forecast. Prophet can model multiplicative seasonality by setting seasonality_mode=&#39;multiplicative&#39; in the model. . | holiday_prior_scale just like changepoint_prior_scale, holiday_prior_scale is used to smoothning the effect of holidays. By default its value is 10, which provides very little regularization. Reducing this parameter dampens holiday effects . | Seasonalities with fourier_order Prophet model, by default finds the seasonalities and adds the default parameters of the seasonality. We can modify the seasonalities effect by adding custom seasonalities as add_seasonality in the model with different fourier order.By default Prophet uses a Fourier order of 3 for weekly seasonality and 10 for yearly seasonality. . | . Hyperparameter tuning using Parameter Grid . from sklearn.model_selection import ParameterGrid params_grid={&#39;seasonality_mode&#39;:(&#39;multiplicative&#39;,&#39;additive&#39;), &#39;changepoint_prior_scale&#39;:[0.1,0.2,0.3,0.4,0.5], &#39;n_changepoints&#39;:[100,150,200]} grid=ParameterGrid(params_grid) cnt=0 for p in grid: cnt+=1 f&quot;the tolal number of possible model combinations are: {cnt}&quot; . . &#39;the tolal number of possible model combinations are: 30&#39; . Model Tuning . start=pd.to_datetime(&#39;2019-12-01&#39;) end=pd.to_datetime(&#39;2020-07-01&#39;) model_parameters= pd.DataFrame(columns=[&#39;MAPE&#39;,&#39;Parameters&#39;]) for p in grid: test= pd.DataFrame() print(p) train_model=Prophet(changepoint_prior_scale=p[&#39;changepoint_prior_scale&#39;], n_changepoints=p[&#39;n_changepoints&#39;], seasonality_mode=p[&#39;seasonality_mode&#39;], weekly_seasonality=True, daily_seasonality=True, yearly_seasonality= True, interval_width=0.95) train_model.fit(X_train) train_forecast= train_model.make_future_dataframe(periods=213, freq=&#39;D&#39;, include_history=False) train_forecast=train_model.predict(train_forecast) test=train_forecast[[&#39;ds&#39;,&#39;yhat&#39;]] Actual=dfd[(dfd[&#39;ds&#39;]&gt;=start) &amp; (dfd[&#39;ds&#39;]&lt;end)] # test=test[(test[&#39;ds&#39;]&gt;=start) | (test[&#39;ds&#39;]&lt;end)] MAPE=mean_absolute_per_error(Actual[&#39;y&#39;],abs(test[&#39;yhat&#39;])) print(&#39; Mean absolute Percentage error-&#39;, MAPE) model_parameters=model_parameters.append({&#39;MAPE&#39;:MAPE, &#39;Parameters&#39;:p}, ignore_index=True) . . {&#39;changepoint_prior_scale&#39;: 0.1, &#39;n_changepoints&#39;: 100, &#39;seasonality_mode&#39;: &#39;multiplicative&#39;} Mean absolute Percentage error- 41.16647872799655 {&#39;changepoint_prior_scale&#39;: 0.1, &#39;n_changepoints&#39;: 100, &#39;seasonality_mode&#39;: &#39;additive&#39;} Mean absolute Percentage error- 79.52174776117857 {&#39;changepoint_prior_scale&#39;: 0.1, &#39;n_changepoints&#39;: 150, &#39;seasonality_mode&#39;: &#39;multiplicative&#39;} Mean absolute Percentage error- 41.31179630741665 {&#39;changepoint_prior_scale&#39;: 0.1, &#39;n_changepoints&#39;: 150, &#39;seasonality_mode&#39;: &#39;additive&#39;} Mean absolute Percentage error- 79.18749961191585 {&#39;changepoint_prior_scale&#39;: 0.1, &#39;n_changepoints&#39;: 200, &#39;seasonality_mode&#39;: &#39;multiplicative&#39;} Mean absolute Percentage error- 40.76211844735218 {&#39;changepoint_prior_scale&#39;: 0.1, &#39;n_changepoints&#39;: 200, &#39;seasonality_mode&#39;: &#39;additive&#39;} Mean absolute Percentage error- 79.66544939372186 {&#39;changepoint_prior_scale&#39;: 0.2, &#39;n_changepoints&#39;: 100, &#39;seasonality_mode&#39;: &#39;multiplicative&#39;} Mean absolute Percentage error- 44.91062871169708 {&#39;changepoint_prior_scale&#39;: 0.2, &#39;n_changepoints&#39;: 100, &#39;seasonality_mode&#39;: &#39;additive&#39;} Mean absolute Percentage error- 67.80063038221549 {&#39;changepoint_prior_scale&#39;: 0.2, &#39;n_changepoints&#39;: 150, &#39;seasonality_mode&#39;: &#39;multiplicative&#39;} Mean absolute Percentage error- 43.11135461131485 {&#39;changepoint_prior_scale&#39;: 0.2, &#39;n_changepoints&#39;: 150, &#39;seasonality_mode&#39;: &#39;additive&#39;} Mean absolute Percentage error- 67.36351872176112 {&#39;changepoint_prior_scale&#39;: 0.2, &#39;n_changepoints&#39;: 200, &#39;seasonality_mode&#39;: &#39;multiplicative&#39;} Mean absolute Percentage error- 41.97917530475909 {&#39;changepoint_prior_scale&#39;: 0.2, &#39;n_changepoints&#39;: 200, &#39;seasonality_mode&#39;: &#39;additive&#39;} Mean absolute Percentage error- 66.67031195166116 {&#39;changepoint_prior_scale&#39;: 0.3, &#39;n_changepoints&#39;: 100, &#39;seasonality_mode&#39;: &#39;multiplicative&#39;} Mean absolute Percentage error- 44.5238946381634 {&#39;changepoint_prior_scale&#39;: 0.3, &#39;n_changepoints&#39;: 100, &#39;seasonality_mode&#39;: &#39;additive&#39;} Mean absolute Percentage error- 62.39730538189566 {&#39;changepoint_prior_scale&#39;: 0.3, &#39;n_changepoints&#39;: 150, &#39;seasonality_mode&#39;: &#39;multiplicative&#39;} Mean absolute Percentage error- 43.20060181983646 {&#39;changepoint_prior_scale&#39;: 0.3, &#39;n_changepoints&#39;: 150, &#39;seasonality_mode&#39;: &#39;additive&#39;} Mean absolute Percentage error- 62.01095821252092 {&#39;changepoint_prior_scale&#39;: 0.3, &#39;n_changepoints&#39;: 200, &#39;seasonality_mode&#39;: &#39;multiplicative&#39;} Mean absolute Percentage error- 41.677471772919375 {&#39;changepoint_prior_scale&#39;: 0.3, &#39;n_changepoints&#39;: 200, &#39;seasonality_mode&#39;: &#39;additive&#39;} Mean absolute Percentage error- 62.26258875274484 {&#39;changepoint_prior_scale&#39;: 0.4, &#39;n_changepoints&#39;: 100, &#39;seasonality_mode&#39;: &#39;multiplicative&#39;} Mean absolute Percentage error- 42.79278862697888 {&#39;changepoint_prior_scale&#39;: 0.4, &#39;n_changepoints&#39;: 100, &#39;seasonality_mode&#39;: &#39;additive&#39;} Mean absolute Percentage error- 61.37076349656182 {&#39;changepoint_prior_scale&#39;: 0.4, &#39;n_changepoints&#39;: 150, &#39;seasonality_mode&#39;: &#39;multiplicative&#39;} Mean absolute Percentage error- 42.00427043800557 {&#39;changepoint_prior_scale&#39;: 0.4, &#39;n_changepoints&#39;: 150, &#39;seasonality_mode&#39;: &#39;additive&#39;} Mean absolute Percentage error- 60.617060245870356 {&#39;changepoint_prior_scale&#39;: 0.4, &#39;n_changepoints&#39;: 200, &#39;seasonality_mode&#39;: &#39;multiplicative&#39;} Mean absolute Percentage error- 42.05727058812413 {&#39;changepoint_prior_scale&#39;: 0.4, &#39;n_changepoints&#39;: 200, &#39;seasonality_mode&#39;: &#39;additive&#39;} Mean absolute Percentage error- 60.69402617698554 {&#39;changepoint_prior_scale&#39;: 0.5, &#39;n_changepoints&#39;: 100, &#39;seasonality_mode&#39;: &#39;multiplicative&#39;} Mean absolute Percentage error- 41.35824639370792 {&#39;changepoint_prior_scale&#39;: 0.5, &#39;n_changepoints&#39;: 100, &#39;seasonality_mode&#39;: &#39;additive&#39;} Mean absolute Percentage error- 60.902825126753534 {&#39;changepoint_prior_scale&#39;: 0.5, &#39;n_changepoints&#39;: 150, &#39;seasonality_mode&#39;: &#39;multiplicative&#39;} Mean absolute Percentage error- 41.00354758113775 {&#39;changepoint_prior_scale&#39;: 0.5, &#39;n_changepoints&#39;: 150, &#39;seasonality_mode&#39;: &#39;additive&#39;} Mean absolute Percentage error- 61.29953997853968 {&#39;changepoint_prior_scale&#39;: 0.5, &#39;n_changepoints&#39;: 200, &#39;seasonality_mode&#39;: &#39;multiplicative&#39;} Mean absolute Percentage error- 39.97532748239143 {&#39;changepoint_prior_scale&#39;: 0.5, &#39;n_changepoints&#39;: 200, &#39;seasonality_mode&#39;: &#39;additive&#39;} Mean absolute Percentage error- 59.847888436385496 . # Filtering the best parameters based on MAPE model_parameters.sort_values(by=&#39;MAPE&#39;).head() . . MAPE Parameters . 28 39.975327 | {&#39;changepoint_prior_scale&#39;: 0.5, &#39;n_changepoin... | . 4 40.762118 | {&#39;changepoint_prior_scale&#39;: 0.1, &#39;n_changepoin... | . 26 41.003548 | {&#39;changepoint_prior_scale&#39;: 0.5, &#39;n_changepoin... | . 0 41.166479 | {&#39;changepoint_prior_scale&#39;: 0.1, &#39;n_changepoin... | . 2 41.311796 | {&#39;changepoint_prior_scale&#39;: 0.1, &#39;n_changepoin... | . f&quot;The best parameters are: {model_parameters.loc[0][&#39;Parameters&#39;]}&quot; . . &#34;The best parameters are: {&#39;changepoint_prior_scale&#39;: 0.1, &#39;n_changepoints&#39;: 100, &#39;seasonality_mode&#39;: &#39;multiplicative&#39;}&#34; . #Using the best parameter for model creation final_model=Prophet(changepoint_prior_scale=0.1, n_changepoints=100, seasonality_mode=&#39;multiplicative&#39;, weekly_seasonality=True, daily_seasonality= True, yearly_seasonality= True, interval_width=0.95) . . final_model.fit(X_train) . . &lt;fbprophet.forecaster.Prophet at 0x7f122c476d00&gt; . future=final_model.make_future_dataframe(periods=602, freq=&#39;D&#39;) forecast=final_model.predict(future) forecast[[&#39;ds&#39;,&#39;yhat&#39;,&#39;yhat_lower&#39;,&#39;yhat_upper&#39;]].tail(10) . . ds yhat yhat_lower yhat_upper . 1999 2020-06-22 | 76.433627 | -791.160734 | 890.188546 | . 2000 2020-06-23 | 76.455268 | -747.205049 | 892.555563 | . 2001 2020-06-24 | 76.617925 | -776.868822 | 899.896378 | . 2002 2020-06-25 | 76.266286 | -784.327136 | 906.923700 | . 2003 2020-06-26 | 77.358265 | -761.259608 | 911.816334 | . 2004 2020-06-27 | 74.105031 | -813.998184 | 873.694340 | . 2005 2020-06-28 | 67.872256 | -684.537000 | 811.292557 | . 2006 2020-06-29 | 63.734660 | -659.921216 | 760.814897 | . 2007 2020-06-30 | 64.098383 | -677.847337 | 776.170516 | . 2008 2020-07-01 | 64.695467 | -671.082923 | 782.237527 | . #plotting the model components fig= final_model.plot_components(forecast) . . # predicting the test values with this model X_test_final= final_model.predict(X_test) X_test_final.tail() . . ds trend yhat_lower yhat_upper trend_lower trend_upper daily daily_lower daily_upper multiplicative_terms ... weekly weekly_lower weekly_upper yearly yearly_lower yearly_upper additive_terms additive_terms_lower additive_terms_upper yhat . 597 2020-06-27 | 12.579281 | -814.152903 | 999.634137 | -132.651057 | 163.091484 | 7.576726 | 7.576726 | 7.576726 | 4.891039 | ... | 0.223225 | 0.223225 | 0.223225 | -2.908912 | -2.908912 | -2.908912 | 0.0 | 0.0 | 0.0 | 74.105031 | . 598 2020-06-28 | 12.558421 | -692.261065 | 882.928529 | -133.091741 | 163.238035 | 7.576726 | 7.576726 | 7.576726 | 4.404522 | ... | -0.134988 | -0.134988 | -0.134988 | -3.037216 | -3.037216 | -3.037216 | 0.0 | 0.0 | 0.0 | 67.872256 | . 599 2020-06-29 | 12.537560 | -686.045239 | 870.195186 | -133.532425 | 163.384666 | 7.576726 | 7.576726 | 7.576726 | 4.083498 | ... | -0.333691 | -0.333691 | -0.333691 | -3.159537 | -3.159537 | -3.159537 | 0.0 | 0.0 | 0.0 | 63.734660 | . 600 2020-06-30 | 12.516700 | -695.981824 | 842.112711 | -133.973110 | 163.549730 | 7.576726 | 7.576726 | 7.576726 | 4.121029 | ... | -0.180807 | -0.180807 | -0.180807 | -3.274890 | -3.274890 | -3.274890 | 0.0 | 0.0 | 0.0 | 64.098383 | . 601 2020-07-01 | 12.495840 | -695.042697 | 894.785314 | -134.413794 | 164.002688 | 7.576726 | 7.576726 | 7.576726 | 4.177360 | ... | -0.016967 | -0.016967 | -0.016967 | -3.382398 | -3.382398 | -3.382398 | 0.0 | 0.0 | 0.0 | 64.695467 | . 5 rows × 22 columns . #calculating the error f&quot;The MAPE (Mean absolute Percentage error) for final Prophet model is: {mean_absolute_per_error(X_test[&#39;y&#39;], X_test_final[&#39;yhat&#39;])}&quot; . . &#39;The MAPE (Mean absolute Percentage error) for final Prophet model is: 28.13650856685177&#39; . The error on the test set has gone down, this indicates better data fittment . #mapping the test and forcast data f,ax= plt.subplots(figsize=(12,5)) X_test.plot(kind=&#39;line&#39;, x=&#39;ds&#39;, y=&#39;y&#39;, label=&quot;Actual&quot;, ax=ax) X_test_final.plot(kind=&#39;line&#39;, x=&#39;ds&#39;, y=&#39;yhat&#39;, label=&quot;forecast&quot;, ax=ax) plt.grid(True) plt.show() . . Time series decomposition . There are three componenets of time series. . Trend: Consistant up/down slope of a time series | Seasonality: Periodic pattern of the time series | Noise: Outliers or missing values | . # Understanding these factors via plotting graph X_train.plot(kind=&#39;line&#39;, x=&#39;ds&#39;, y=&#39;y&#39;) plt.title(&quot;Plot of AQI&quot;) plt.grid(True) . . #breaking the plot in different sub components import statsmodels.api as sm decomposed_AQI=sm.tsa.seasonal_decompose(X_train[&#39;y&#39;], freq=360) decomposed_AQI.plot() . . There is clear downword trend in the time series | There is an yearly seasonality | Resources Used: . Kaggle notebook: Time series using Prophet | Guide to time series analysis &amp; Forecasting | Tutorial point tutorial on Time series | Everything you can do with time series .",
            "url": "http://kaushalbundel.page/pollution/time-series/pandas/visualization/prophet/2020/12/02/Time_Series_analysis_delhi.html",
            "relUrl": "/pollution/time-series/pandas/visualization/prophet/2020/12/02/Time_Series_analysis_delhi.html",
            "date": " • Dec 2, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Understanding Wealth Mindset and its difference with Status Mindset",
            "content": "Understanding Wealth Mindset and its difference with Status Mindset . I had seen the terms “Wealth” and “Status” on an episode of BeerBiceps Podcast by Ranveer Allahbadia with Kunal Shah of cred frame. . Youtube link of the Podcast . The topic was quite interesting with Kunal Shah explaining that there are two kinds of societies, wealth driven and status driven and there is a lot of difference between these two types of socities. The topic was covered very briefly and more context was derived by reading Kunal’s Twitter timeline. . . |Wealth driven society | Status Driven Society| |-|-| |- more colleborative society|- less colleborative Society| |- is not zero sum|- is zero sum| |- Wealth Generation is rewarded|-Wealth Generation is abhored| |- Systematic due to positive sum mindset|- Chaotic due to zero sum mindset| |- has democratic leaders|- prefers authoritarian leadership| |- focus on quality/value worship|- focus on icon/hero worship| |- Celebrate risk takers as they are trying to do something different|- Destest risk taking as it is detrimental to status quo| |- Novalty is a feature|- Novelty is a bug| — . How I see status and wealth driven mindset . The points that are specified for society can be similarly applied for mindset as well. A person who is living in a status driven society is more susceptable to be inflicted with status driven mindset and vice versa. What’s important here is to evaluate ones actions in terms of “status driven” or “wealth driven”. The final objective of the evaluation is to think in such as way as to habituate the wealth driven actions and deloop status driven actions from day to day routine. .",
            "url": "http://kaushalbundel.page/funda/status/value/mindset/2020/11/30/Wealth-vs-Status.html",
            "relUrl": "/funda/status/value/mindset/2020/11/30/Wealth-vs-Status.html",
            "date": " • Nov 30, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Understanding Air pollution",
            "content": "The winter season here in Northern part of the country and with this we have the call of pollution/smog in news and in life all around us. Many of my friends and relatives are suffering from the ill effects of hightened air pollution in these times. This write up is my effort in understanding the trend of air pollution, with a slight degression on the possible causes and consitituents of air pollution. . Small description about the constituents which make up air pollution around us: . PM (Particulate Matter):PM is a mixture of solid and liqued particles that are suspended in the air. IARC and WHO designate these airborne particulates as Group 1 carcirogen. Particulates are most harmful form of air pollution due to their ability to penetrate deep into lungs, blood streams and brain causing health problems including heart attacks, respiratory diseases and premature death.Some particulates originate naturally from forest fires, volcanoes, dust storms and living vegetation. Human activities such as buring of fossil fuels in vehicles, stubble burning, road dust and power plants are some of the man made source of PM. The amount of time a particle can be suspended in the air depends upon the size, Large particles (&gt;10 micrometers) tend to settle on the ground by gravity in a matter of hours while smaller particles can be suspended in air for weeks and can mostly be removed by precipitation. PM2.5: PM2.5 are fine particles with diameter of 2.5 micrometers. These particles can be made up from different chemicals, Some are emitted directly from a source such as constuction sites, unpaved roads, fields, crackers or fires. | PM10: This size of Particulate matter is small enough to go into lungs and throat. High levels of PM10 can make you cough, your nose run and eyes sting. | . | NOx: NOx is a generic term for Nitrogen oxides that are most relevant for air pollution. Nitrogen Oxides are expelled from high temperature combustion, and are also produced during thunderstorms by electric discharge. They can be seen as a brown haze dome above or a plume downwind of cities. NOx gases are usually produced from the reaction between Nitrogen and Oxygen during combustion of fuels (in car engines).NOx respiratory exposure can trigger and exacerbate existing asthma symptoms, and may even lead to the development of asthma over longer periods of time. It has also been associated with heart disease, diabetes, birth outcomes, and all-cause mortality, but these nonrespiratory effects are less well-established. | NH3 (Ammonia): Ammonia (NH3) is a highly reactive and soluble alkaline gas. It originates from both natural and human generated sources, with the main source being agriculture, e.g. manures, slurries and fertiliser application.Ammonia is also emitted from a range of non-agricultural sources, such as catalytic converters in petrol cars, landfill sites, sewage works, composting of organic materials, combustion, industry and wild mammals and birds. Exposure to high concentrations of ammonia in air causes immediate burning of the eyes, nose, throat and respiratory tract and can result in blindness, lung damage or death. Inhalation of lower concentrations can cause coughing, and nose and throat irritation. | CO (Carbon Monooxide): Carbon monoxide (CO)—a colorless, odorless, tasteless, and toxic air pollutant—is produced in the incomplete combustion of carbon-containing fuels, such as gasoline, natural gas, oil, coal, and wood. | SO2: Sulphur Dioxide: Sulfur dioxide (SO2), a colorless, bad-smelling, toxic gas, is part of a larger group of chemicals referred to as sulfur oxides (SOx). These gases, especially SO2, are emitted by the burning of fossil fuels — coal, oil, and diesel — or other materials that contain sulfur. Sources include power plants, metals processing and smelting facilities, and vehicles. Diesel vehicles and equipment have long been a major source of sulfur dioxide. Like nitrogen dioxide, sulfur dioxide can create secondary pollutants once released into the air. Secondary pollutants formed with sulfur dioxide include sulfate aerosols, particulate matter, and acid rain. Sulfur dioxide, associated SOx, and secondary pollutants can contribute to respiratory illness by making breathing more difficult, especially for children, the elderly, and those with pre-existing conditions. Longer exposures can aggravate existing heart and lung conditions, as well. Sulfur dioxide and other SOx are partly culpable in the formation of thick haze and smog, which can impair visibility in addition to impacting health. Beyond human health impacts, sulfur dioxide’s contribution to acid rain can cause direct harm to trees and plants by damaging exposed tissues and, subsequently, decreasing plant growth. Other sensitive ecosystems and waterways are also impacted by acid rain. | O3: Ozone is a gas composed of three atoms of oxygen (O3). Ozone occurs both in the Earth&#39;s upper atmosphere and at ground level. Ozone can be good or bad, depending on where it is found. Good Ozone called stratospheric ozone, occurs naturally in the upper atmosphere, where it forms a protective layer that shields us from the sun&#39;s harmful ultraviolet rays. Tropospheric, or ground level ozone, is a harmful air pollutant, because of its effects on people and the environment, and it is the main ingredient in “smog&quot;. Tropospheric Ozone, is created by chemical reactions between oxides of nitrogen (NOx) and volatile organic compounds (VOC). This happens when pollutants emitted by cars, power plants, industrial boilers, refineries, chemical plants, and other sources chemically react in the presence of sunlight. Ozone in the air we breathe can harm our health. People most at risk from breathing air containing ozone include people with asthma, children, older adults, and people who are active outdoors, especially outdoor workers. In addition, people with certain genetic characteristics, and people with reduced intake of certain nutrients, such as vitamins C and E, are at greater risk from ozone exposure. | Benzene: The main sources of benzene are from vehicle exhaust and other combustion processes and from industry producing or using it. Benzene is also released naturally from volcanoes and forest fires, but the amounts released are insignificant in comparison to those emitted by man&#39;s activities. Benzene is a proven carcinogen. However, exposure to normal environmental concentrations in air (from the vapourisation of petrol during re-fuelling of vehicles, from tobacco smoke, glues, paint, furniture wax and detergents) is thought unlikely to be dangerous in this respect. Inhalation of extremely high levels of benzene (following an accidental releae) could be fatal and longer term exposure to lower concentrations (in occupational settings for example) may damage blood-forming organs. When ingested or applied directly to the skin (only likely in occupational settings), benzene is very toxic. Inhalation of ground level ozone (in the formation of which benzene can be involved) can exacerbate respiratory conditions such as asthma. | Toulene: Toluene is a clear, colorless liquid with a distinctive smell. It is a good solvent (a substance that can dissolve other substances). Toluene occurs naturally in crude oil and in the tolu tree. It is produced in the process of making gasoline and other fuels from crude oil and in making coke from coal.Toluene is a clear, colorless liquid with a distinctive smell. It is a good solvent (a substance that can dissolve other substances). Toluene occurs naturally in crude oil and in the tolu tree. It is produced in the process of making gasoline and other fuels from crude oil and in making coke from coal.A serious health concern is that toluene may have an effect on your nervous system (brain and nerves). Nervous system effects can be temporary, such as headaches, dizziness, or unconsciousness. However, effects such as incoordination, cognitive impairment, and vision and hearing loss may become permanent with repeated exposure, especially at concentrations associated with intentional solvent abuse. ow to moderate, day-after-day exposure to toluene in your workplace can cause tiredness, confusion, weakness, drunken-type actions, memory loss, nausea, and loss of appetite. These symptoms usually disappear when exposure is stopped. You may experience some hearing and color vision loss after long-term daily exposure to toluene in the workplace. | Xylene: Xylene is an aromatic hydrocarbon widely used in industry and medical technology as a solvent. It is a colorless, sweet-smelling liquid or gas occurring naturally in petroleum, coal and wood tar, and is so named because it is found in crude wood spirit. Xylene is primarily released from industrial sources, in automobile exhaust, and during its use as a solvent. Short-term exposure of people to high levels of xylene can cause irritation of the skin, eyes, nose, and throat; difficulty in breathing; impaired function of the lungs; delayed response to a visual stimulus; impaired memory; stomach discomfort; and possible changes in the liver and kidneys. Both short- and long-term exposure to high concentrations of xylene can also cause a number of effects on the nervous system, such as headaches, lack of muscle coordination, dizziness, confusion, and changes in one&#39;s sense of balance. | . Data Loading . # importing libraries import pandas as pd import os import matplotlib.pyplot as plt import altair as alt import numpy as np . . # reading the file df=pd.read_csv(&quot;city_day.csv&quot;) . . df.head() . City Date PM2.5 PM10 NO NO2 NOx NH3 CO SO2 O3 Benzene Toluene Xylene AQI AQI_Bucket . 0 Ahmedabad | 2015-01-01 | NaN | NaN | 0.92 | 18.22 | 17.15 | NaN | 0.92 | 27.64 | 133.36 | 0.00 | 0.02 | 0.00 | NaN | NaN | . 1 Ahmedabad | 2015-01-02 | NaN | NaN | 0.97 | 15.69 | 16.46 | NaN | 0.97 | 24.55 | 34.06 | 3.68 | 5.50 | 3.77 | NaN | NaN | . 2 Ahmedabad | 2015-01-03 | NaN | NaN | 17.40 | 19.30 | 29.70 | NaN | 17.40 | 29.07 | 30.70 | 6.80 | 16.40 | 2.25 | NaN | NaN | . 3 Ahmedabad | 2015-01-04 | NaN | NaN | 1.70 | 18.48 | 17.97 | NaN | 1.70 | 18.59 | 36.08 | 4.43 | 10.14 | 1.00 | NaN | NaN | . 4 Ahmedabad | 2015-01-05 | NaN | NaN | 22.10 | 21.42 | 37.76 | NaN | 22.10 | 39.33 | 39.31 | 7.01 | 18.89 | 2.78 | NaN | NaN | . Preparing the data . #Converting the object datetime to pandas datetime #We need to do this convesrion to enable time series data ploting df= df.rename({&#39;PM2.5&#39;:&#39;PM&#39;}, axis=1) df[&#39;Date&#39;]=pd.to_datetime(df[&#39;Date&#39;]) . . city_list=df[&#39;City&#39;].unique() f&quot;The cities under consideration of this study are:&quot;,&#39;, &#39;.join(city_list) . . (&#39;The cities under consideration of this study are:&#39;, &#39;Ahmedabad, Aizawl, Amaravati, Amritsar, Bengaluru, Bhopal, Brajrajnagar, Chandigarh, Chennai, Coimbatore, Delhi, Ernakulam, Gurugram, Guwahati, Hyderabad, Jaipur, Jorapokhar, Kochi, Kolkata, Lucknow, Mumbai, Patna, Shillong, Talcher, Thiruvananthapuram, Visakhapatnam&#39;) . df.head() . . City Date PM PM10 NO NO2 NOx NH3 CO SO2 O3 Benzene Toluene Xylene AQI AQI_Bucket . 0 Ahmedabad | 2015-01-01 | NaN | NaN | 0.92 | 18.22 | 17.15 | NaN | 0.92 | 27.64 | 133.36 | 0.00 | 0.02 | 0.00 | NaN | NaN | . 1 Ahmedabad | 2015-01-02 | NaN | NaN | 0.97 | 15.69 | 16.46 | NaN | 0.97 | 24.55 | 34.06 | 3.68 | 5.50 | 3.77 | NaN | NaN | . 2 Ahmedabad | 2015-01-03 | NaN | NaN | 17.40 | 19.30 | 29.70 | NaN | 17.40 | 29.07 | 30.70 | 6.80 | 16.40 | 2.25 | NaN | NaN | . 3 Ahmedabad | 2015-01-04 | NaN | NaN | 1.70 | 18.48 | 17.97 | NaN | 1.70 | 18.59 | 36.08 | 4.43 | 10.14 | 1.00 | NaN | NaN | . 4 Ahmedabad | 2015-01-05 | NaN | NaN | 22.10 | 21.42 | 37.76 | NaN | 22.10 | 39.33 | 39.31 | 7.01 | 18.89 | 2.78 | NaN | NaN | . Analysis of PAN India data . Lets try to understand how air pollutant are spread PAN India . # Total country performance ind=df[&#39;AQI_Bucket&#39;].value_counts() ind= pd.DataFrame(ind, columns=[&#39;AQI_Bucket&#39;]) ind[&#39;per&#39;]=(ind[&#39;AQI_Bucket&#39;]/ (ind[&#39;AQI_Bucket&#39;].sum()))*100 ind . . AQI_Bucket per . Moderate 8829 | 35.529175 | . Satisfactory 8224 | 33.094567 | . Poor 2781 | 11.191147 | . Very Poor 2337 | 9.404427 | . Good 1341 | 5.396378 | . Severe 1338 | 5.384306 | . f&quot;Out of total captured days ({ind[&#39;AQI_Bucket&#39;].sum()}), around {ind.loc[[&#39;Poor&#39;,&#39;Very Poor&#39;,&#39;Severe&#39;]][&#39;per&#39;].sum():.2f}% have bad air quality&quot; . . &#39;Out of total captured days (24850), around 25.98% have bad air quality&#39; . City wise Breakup of AQI across India . In this section, we need to understand how AQI Index is distributed across India. . city_AQI=df.groupby(by=&#39;City&#39;)[&#39;AQI_Bucket&#39;].value_counts() . . city_list= df[&#39;City&#39;].unique() . . for city in city_list: print(f&#39;City Name:{city} and AQI Index n{city_AQI[city]}&#39;) . . City Name:Ahmedabad and AQI Index AQI_Bucket Severe 638 Poor 238 Very Poor 216 Moderate 198 Satisfactory 43 Good 1 Name: AQI_Bucket, dtype: int64 City Name:Aizawl and AQI Index AQI_Bucket Good 83 Satisfactory 28 Name: AQI_Bucket, dtype: int64 City Name:Amaravati and AQI Index AQI_Bucket Satisfactory 409 Moderate 219 Good 158 Poor 47 Very Poor 8 Name: AQI_Bucket, dtype: int64 City Name:Amritsar and AQI Index AQI_Bucket Satisfactory 473 Moderate 448 Good 84 Poor 67 Very Poor 47 Severe 7 Name: AQI_Bucket, dtype: int64 City Name:Bengaluru and AQI Index AQI_Bucket Satisfactory 1124 Moderate 630 Good 115 Poor 36 Very Poor 5 Name: AQI_Bucket, dtype: int64 City Name:Bhopal and AQI Index AQI_Bucket Moderate 165 Satisfactory 76 Poor 31 Very Poor 4 Good 2 Name: AQI_Bucket, dtype: int64 City Name:Brajrajnagar and AQI Index AQI_Bucket Moderate 421 Satisfactory 122 Poor 120 Very Poor 36 Good 14 Name: AQI_Bucket, dtype: int64 City Name:Chandigarh and AQI Index AQI_Bucket Satisfactory 154 Moderate 74 Good 48 Poor 20 Very Poor 3 Name: AQI_Bucket, dtype: int64 City Name:Chennai and AQI Index AQI_Bucket Satisfactory 941 Moderate 804 Poor 110 Good 12 Very Poor 11 Severe 6 Name: AQI_Bucket, dtype: int64 City Name:Coimbatore and AQI Index AQI_Bucket Satisfactory 288 Good 43 Moderate 13 Name: AQI_Bucket, dtype: int64 City Name:Delhi and AQI Index AQI_Bucket Poor 542 Very Poor 520 Moderate 519 Severe 239 Satisfactory 158 Good 21 Name: AQI_Bucket, dtype: int64 City Name:Ernakulam and AQI Index AQI_Bucket Satisfactory 104 Moderate 49 Name: AQI_Bucket, dtype: int64 City Name:Gurugram and AQI Index AQI_Bucket Moderate 454 Very Poor 348 Poor 310 Satisfactory 225 Severe 95 Good 21 Name: AQI_Bucket, dtype: int64 City Name:Guwahati and AQI Index AQI_Bucket Satisfactory 135 Good 116 Moderate 111 Poor 69 Very Poor 59 Severe 5 Name: AQI_Bucket, dtype: int64 City Name:Hyderabad and AQI Index AQI_Bucket Moderate 958 Satisfactory 710 Good 141 Poor 54 Very Poor 10 Severe 7 Name: AQI_Bucket, dtype: int64 City Name:Jaipur and AQI Index AQI_Bucket Moderate 651 Satisfactory 317 Poor 110 Very Poor 10 Good 4 Severe 2 Name: AQI_Bucket, dtype: int64 City Name:Jorapokhar and AQI Index AQI_Bucket Moderate 451 Satisfactory 137 Poor 121 Very Poor 34 Severe 18 Good 10 Name: AQI_Bucket, dtype: int64 City Name:Kochi and AQI Index AQI_Bucket Satisfactory 82 Moderate 74 Poor 2 Name: AQI_Bucket, dtype: int64 City Name:Kolkata and AQI Index AQI_Bucket Satisfactory 285 Moderate 152 Good 119 Poor 119 Very Poor 66 Severe 13 Name: AQI_Bucket, dtype: int64 City Name:Lucknow and AQI Index AQI_Bucket Moderate 578 Very Poor 473 Satisfactory 365 Poor 352 Severe 110 Good 15 Name: AQI_Bucket, dtype: int64 City Name:Mumbai and AQI Index AQI_Bucket Satisfactory 428 Moderate 285 Poor 35 Good 26 Very Poor 1 Name: AQI_Bucket, dtype: int64 City Name:Patna and AQI Index AQI_Bucket Moderate 507 Very Poor 371 Poor 236 Severe 174 Satisfactory 171 Name: AQI_Bucket, dtype: int64 City Name:Shillong and AQI Index AQI_Bucket Good 111 Satisfactory 84 Moderate 10 Name: AQI_Bucket, dtype: int64 City Name:Talcher and AQI Index AQI_Bucket Moderate 324 Satisfactory 150 Very Poor 97 Poor 84 Severe 24 Good 19 Name: AQI_Bucket, dtype: int64 City Name:Thiruvananthapuram and AQI Index AQI_Bucket Satisfactory 768 Moderate 154 Good 126 Poor 4 Name: AQI_Bucket, dtype: int64 City Name:Visakhapatnam and AQI Index AQI_Bucket Moderate 580 Satisfactory 447 Poor 74 Good 52 Very Poor 18 Name: AQI_Bucket, dtype: int64 . Based on the above analysis, we found out that Air Quality for Delhi is worst in all the cities under study. . We can also conclude that in general Tier 2 and Tier 3 cities have much better air quality. Aizwal and Shillong have the best air quality as compared to other cities in the study. . Year wise analysis- Pan India Data . # Adding month and year into the dataset so that we can analyse month and year wise trends df1=df.copy() df1[&#39;year&#39;]=df1[&#39;Date&#39;].dt.year df1[&#39;month&#39;]=df1[&#39;Date&#39;].dt.month . . year=df1.groupby(&#39;year&#39;)[[&#39;PM&#39;,&#39;PM10&#39;, &#39;NO&#39;, &#39;NO2&#39;, &#39;NOx&#39;, &#39;NH3&#39;, &#39;CO&#39;, &#39;SO2&#39;,&#39;O3&#39;, &#39;Benzene&#39;, &#39;Toluene&#39;,&#39;AQI&#39;]].mean().reset_index() f&quot;The average concertration of pollutants across different years is:&quot;,(year[0:5]) . . (&#39;The average concertration of pollutants across different years is:&#39;, year PM PM10 NO NO2 NOx NH3 0 2015 83.323750 186.018164 16.588714 23.729427 35.541396 37.713707 1 2016 92.223729 145.500156 19.408612 31.362462 30.861736 39.499053 2 2017 86.034757 129.082620 17.666010 31.963634 24.023689 30.365794 3 2018 69.230657 133.263001 18.813078 33.353737 37.169719 21.890958 4 2019 58.958241 112.964335 16.443662 28.497536 33.337306 18.965398 CO SO2 O3 Benzene Toluene AQI 0 4.205297 13.513310 32.919829 4.290253 7.150069 212.463054 1 1.807855 10.063124 38.121916 2.330611 5.605426 197.150019 2 1.256459 12.225970 34.409825 1.600159 4.477028 181.472789 3 3.004921 18.171016 35.221742 2.692337 11.085401 182.684312 4 2.298988 16.353351 33.051420 3.298604 10.916312 156.518173 ) . In the above case I have not considered the year 2020 as the data is incomplete. The year does not include winter months as the pollution is maximum during this period. . pollutants=[&#39;PM&#39;,&#39;PM10&#39;, &#39;NO&#39;, &#39;NO2&#39;, &#39;NOx&#39;, &#39;NH3&#39;, &#39;CO&#39;, &#39;SO2&#39;,&#39;O3&#39;, &#39;Benzene&#39;, &#39;Toluene&#39;, &#39;AQI&#39;] . . #Creating a function to plot the data using altair def plot_trail(data,x,y, size): base=alt.Chart(data).properties(width=800) return base.mark_trail().encode(x=x,y=y,size=y, tooltip=y).interactive() . . #plotting the data for i in pollutants: chart=plot_trail(data=year, x=&quot;year&quot;, y=i, size=i) chart.display() . . The above is a collection of simple graphs which show the long term trend of pollutants spanned across years from 2015 to 2019. The general trend is decreasing pollutants except NOx and Toulene. . Month wise Analysis: Pan India . There is a general awareness that the winter months have more pollution generally as compared to hot summer period. Lets test this assumption. . # Converting Date into month month=df1.groupby(&#39;month&#39;)[[&#39;PM&#39;,&#39;PM10&#39;, &#39;NO&#39;, &#39;NO2&#39;, &#39;NOx&#39;, &#39;NH3&#39;, &#39;CO&#39;, &#39;SO2&#39;,&#39;O3&#39;, &#39;Benzene&#39;, &#39;Toluene&#39;,&#39;AQI&#39;]].mean().reset_index() month.head() . . month PM PM10 NO NO2 NOx NH3 CO SO2 O3 Benzene Toluene AQI . 0 1 | 107.757661 | 166.293820 | 24.600325 | 36.447812 | 43.785974 | 30.247974 | 3.043315 | 17.842014 | 37.278333 | 4.180677 | 10.555584 | 231.674918 | . 1 2 | 82.897863 | 147.695830 | 21.872488 | 34.247877 | 41.045947 | 25.783539 | 2.915327 | 16.829378 | 38.102372 | 3.861679 | 8.972077 | 202.905197 | . 2 3 | 63.933472 | 116.806991 | 16.716655 | 27.662019 | 32.297255 | 21.994147 | 2.195853 | 15.656436 | 38.298299 | 5.962314 | 10.946681 | 164.735281 | . 3 4 | 52.636271 | 105.590680 | 14.346990 | 24.442319 | 25.491347 | 20.741337 | 1.671886 | 14.202350 | 39.002945 | 2.046507 | 5.590473 | 143.355120 | . 4 5 | 51.305960 | 105.301199 | 13.186773 | 22.830505 | 23.591067 | 20.468013 | 1.406012 | 14.299050 | 39.926681 | 2.646210 | 7.391661 | 135.489579 | . #plotting the graphs for i in pollutants: chart=plot_trail(data=month, x=&quot;month&quot;, y=i, size=i) chart.display() . . As it is shown from the graphs above, the pollutants increase during the winter months and decrease during summer months. . A comparison of a landlocked city and ocean facing city . Here I will be comparing two different cities Delhi and Mumbai, Delhi and mumbai have similar demographies with Delhi being the capital of India and Mumbai being the financial captial of India. The two cities are similar in terms of population, with the major geographical difference that Delhi is landlocked from all sides and mumbai has Arbian sea on its western end . dm= df1.set_index(&quot;City&quot;).loc[[&#39;Delhi&#39;,&#39;Mumbai&#39;]] # Dataframe which includes only Delhi and Mumbai as cities . . dm.pivot_table(index=[&#39;City&#39;,&#39;AQI_Bucket&#39;],columns=&#39;year&#39;, values=[&#39;month&#39;], aggfunc={&quot;month&quot;:&#39;count&#39;}, fill_value=&quot;Data Not Avilable&quot;) . . month . year 2015 2016 2017 2018 2019 2020 . City AQI_Bucket . Delhi Good Data Not Avilable | Data Not Avilable | 21 | Data Not Avilable | Data Not Avilable | Data Not Avilable | . Moderate 51 | 83 | 54 | 105 | 128 | 98 | . Poor 130 | 105 | 68 | 105 | 99 | 35 | . Satisfactory 1 | 4 | 50 | 35 | 43 | 25 | . Severe 33 | 85 | 44 | 38 | 36 | 3 | . Very Poor 150 | 88 | 119 | 82 | 59 | 22 | . Mumbai Good Data Not Avilable | Data Not Avilable | Data Not Avilable | 1 | 10 | 15 | . Moderate Data Not Avilable | Data Not Avilable | Data Not Avilable | 93 | 133 | 59 | . Poor Data Not Avilable | Data Not Avilable | Data Not Avilable | 3 | 20 | 12 | . Satisfactory Data Not Avilable | Data Not Avilable | Data Not Avilable | 129 | 202 | 97 | . Very Poor Data Not Avilable | Data Not Avilable | Data Not Avilable | 1 | Data Not Avilable | Data Not Avilable | . As we can see from this data, that Mumbai did not have any days where the air quality is severe, Also for majority of the time the air quality ranges from moderate to satifactory for the case of Mumbai whereas for the case of Delhi the air quality majorily lies between Moderate to very poor. . Checking the PM density for both Mumbai and delhi . dm.pivot_table(index=[&#39;City&#39;,&#39;AQI_Bucket&#39;],columns=&#39;year&#39;, values=[&#39;PM&#39;], aggfunc={&quot;PM&quot;:np.mean}, fill_value=&quot;Data Not Available&quot;) . . PM . year 2015 2016 2017 2018 2019 2020 . City AQI_Bucket . Delhi Good Data Not Available | Data Not Available | 24.7165 | Data Not Available | Data Not Available | Data Not Available | . Moderate 52.0778 | 54.3948 | 66.4422 | 55.7992 | 59.0817 | 55.0774 | . Poor 81.597 | 87.1392 | 103.796 | 103.606 | 97.7254 | 108.023 | . Satisfactory 34.89 | 42.035 | 38.9318 | 31.8414 | 27.7247 | 31.9072 | . Severe 236.617 | 274.652 | 266.548 | 264.749 | 305.273 | 309.36 | . Very Poor 144.817 | 152.894 | 170.622 | 171.58 | 172.607 | 172.456 | . Mumbai Good Data Not Available | Data Not Available | Data Not Available | 16.22 | 8.747 | 10.4847 | . Moderate Data Not Available | Data Not Available | Data Not Available | 51.4305 | 55.1386 | 59.9103 | . Poor Data Not Available | Data Not Available | Data Not Available | 105.23 | 93.716 | 96.1292 | . Satisfactory Data Not Available | Data Not Available | Data Not Available | 20.6702 | 16.7511 | 19.0848 | . Very Poor Data Not Available | Data Not Available | Data Not Available | 122.18 | Data Not Available | Data Not Available | . As it can be seen here that even on good/Satisfactory days more particle matter is present in air for Delhi vis a vis Mumbai. In essence Mumbai has much better quality air as compared to delhi. This can be also attributed to the fact that Mumbai is located close to the sea line. . Gauging the effect of lockdown on pollution . India had a 4 month complete lockdown period due to covid. Generally during this time pollution levels dropped quite low. . ym=df1.groupby([&#39;year&#39;,&#39;month&#39;])[&#39;AQI&#39;].mean().reset_index() . . data=ym[ym[&#39;month&#39;]&lt;=7] . . alt.Chart(data).mark_line().encode( x=&#39;month:O&#39;, y=&#39;AQI:Q&#39;, color=&#39;year:O&#39;,strokeDash=&#39;year:O&#39;).properties(width=500) . . As can be seen from the graph that the AQI is much better for the months post March. Thus the lockdown has had a positive impact on the air quality. . Analysis of Jaipur city . #Checking the air quality index for Jaipur by seggregating Jaipur as cityy jpr=df[df[&#39;City&#39;]==&quot;Jaipur&quot;] jpr=jpr.drop([&#39;Xylene&#39;], axis=1) #The master data does not capture values for Xylene. Dropping the Xylene coloumn . . #Now plotting the data for every pollutant for i in pollutants: chart=plot_trail(data=jpr,x=&#39;Date&#39;,y=i,size=i) chart.display() . . Jaipur also follows the trend where the air pollution increases during winter months. The AQI amd PM10 is also high during summer months due to intense dust storms. . Air Quality Serverity Analysis: Based on Air Quality Standards specified by Govt. of India . per_PM= 40 #annual mean per_Pm10=60 # annual mean per_O3= 100 # 8 hour mean per_NO2= 40 # annual mean per_SO2=50 # 24 hour mean per_CO= 2 #8 hours per_NH3= 100 # annual mean per_Benzene= 5 #annual mean . # removing the nan values jpr_wn=jpr[jpr[&#39;Date&#39;]&gt;=&#39;2017-06-20&#39;] . . jpr_pm=jpr_wn[[&#39;Date&#39;,&#39;PM&#39;]] jpr_pm10=jpr_wn[[&#39;Date&#39;,&#39;PM10&#39;]] jpr_o3=jpr_wn[[&#39;Date&#39;,&#39;O3&#39;]] jpr_no2=jpr_wn[[&#39;Date&#39;,&#39;NO2&#39;]] jpr_so2=jpr_wn[[&#39;Date&#39;,&#39;SO2&#39;]] jpr_co=jpr_wn[[&#39;Date&#39;,&#39;CO&#39;]] jpr_nh3=jpr_wn[[&#39;Date&#39;,&#39;NH3&#39;]] jpr_benzene=jpr_wn[[&#39;Date&#39;,&#39;Benzene&#39;]] . . jpr.head() . . City Date PM PM10 NO NO2 NOx NH3 CO SO2 O3 Benzene Toluene AQI AQI_Bucket . 16587 Jaipur | 2017-06-14 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 16588 Jaipur | 2017-06-15 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 16589 Jaipur | 2017-06-16 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 16590 Jaipur | 2017-06-17 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 16591 Jaipur | 2017-06-18 | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . bars= alt.Chart(title=&quot;PM concentration in Jaipur&quot;, width=1000).mark_bar().encode(x=&#39;Date:T&#39;, y=&#39;PM&#39;) overlay= pd.DataFrame({&#39;x&#39;:[per_PM]}) vline=alt.Chart(overlay).mark_rule(color=&#39;red&#39;, strokeWidth=3).encode(y=&#39;x&#39;) alt.layer(bars, vline, data=jpr_pm) . . As can be seen from the above graph the PM 2.5 concentration is consistantly high above the recommended le . bars= alt.Chart(title=&quot;PM10 concentration in Jaipur&quot;, width=1000).mark_bar().encode(x=&#39;Date:T&#39;, y=&#39;PM10&#39;) overlay= pd.DataFrame({&#39;x&#39;:[per_Pm10]}) vline=alt.Chart(overlay).mark_rule(color=&#39;red&#39;, strokeWidth=3).encode(y=&#39;x&#39;) alt.layer(bars, vline, data=jpr_pm10) . . This concentration is also very high for most of the days in Jaipur . bars= alt.Chart(title=&quot;Ozone concentration in Jaipur&quot;, width=1000).mark_bar().encode(x=&#39;Date:T&#39;, y=&#39;O3&#39;) overlay= pd.DataFrame({&#39;x&#39;:[per_O3]}) vline=alt.Chart(overlay).mark_rule(color=&#39;red&#39;, strokeWidth=3).encode(y=&#39;x&#39;) alt.layer(bars, vline, data=jpr_o3) . . Except for some months in 2018 O3 level is pretty much regulated in Jaipur . bars= alt.Chart(title=&quot;NO2 concentration in Jaipur&quot;, width=1000).mark_bar().encode(x=&#39;Date:T&#39;, y=&#39;NO2&#39;) overlay= pd.DataFrame({&#39;x&#39;:[per_NO2]}) vline=alt.Chart(overlay).mark_rule(color=&#39;red&#39;, strokeWidth=3).encode(y=&#39;x&#39;) alt.layer(bars, vline, data=jpr_no2) . . This graphs shows that NO2 level is above the limit for most of the cases and a drop in NO2 Levels can be seen during summers . bars= alt.Chart(title=&quot;SO2 concentration in Jaipur&quot;, width=1000).mark_bar().encode(x=&#39;Date:T&#39;, y=&#39;SO2&#39;) overlay= pd.DataFrame({&#39;x&#39;:[per_SO2]}) vline=alt.Chart(overlay).mark_rule(color=&#39;red&#39;, strokeWidth=3).encode(y=&#39;x&#39;) alt.layer(bars, vline, data=jpr_so2) . . Finally a metric in which the pollutant is below the permissible level. . bars= alt.Chart(title=&quot;CO concentration in Jaipur&quot;, width=1000).mark_bar().encode(x=&#39;Date:T&#39;, y=&#39;CO&#39;) overlay= pd.DataFrame({&#39;x&#39;:[per_CO]}) vline=alt.Chart(overlay).mark_rule(color=&#39;red&#39;, strokeWidth=3).encode(y=&#39;x&#39;) alt.layer(bars, vline, data=jpr_co) . . Barring some days during winter months the concentration of CO is lower than permissible level. . bars= alt.Chart(title=&quot;Ammonia concentration in Jaipur&quot;, width=1000).mark_bar().encode(x=&#39;Date:T&#39;, y=&#39;NH3&#39;) overlay= pd.DataFrame({&#39;x&#39;:[per_NH3]}) vline=alt.Chart(overlay).mark_rule(color=&#39;red&#39;, strokeWidth=3).encode(y=&#39;x&#39;) alt.layer(bars, vline, data=jpr_nh3) . . Ammonia concerntration is also much lower than the permissible levels . bars= alt.Chart(title=&quot;Benzene concentration in Jaipur&quot;, width=1000).mark_bar().encode(x=&#39;Date:T&#39;, y=&#39;Benzene&#39;) overlay= pd.DataFrame({&#39;x&#39;:[per_Benzene]}) vline=alt.Chart(overlay).mark_rule(color=&#39;red&#39;, strokeWidth=3).encode(y=&#39;x&#39;) alt.layer(bars, vline, data=jpr_benzene) . . The winter trend is again repeated here where the concentration of Benzene increases during winter months and goes down during summers . References: (https://waqi.info/) (https://www.health.ny.gov/environmental/indoors/air/pmq_a.htm) (https://en.wikipedia.org/wiki/Particulates) (https://en.wikipedia.org/wiki/NOx) (https://www.health.ny.gov/environmental/emergency/chemical_terrorism/ammonia_general.htm) (https://www2.sepa.org.uk/SPRIPA/Pages/SubstanceInformation.aspx?pid=21) (https://www.atsdr.cdc.gov/phs/phs.asp?id=159&amp;tid=29) (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2996004/) (https://www.atsdr.cdc.gov/phs/phs.asp?id=293&amp;tid=53) (https://en.wikipedia.org/wiki/Air_quality_guideline) (https://en.wikipedia.org/wiki/Air_quality_guideline) (http://www.arthapedia.in/index.php?title=Ambient_Air_Quality_Standards_in_India) .",
            "url": "http://kaushalbundel.page/pollution/altair/pandas/visualization/2020/11/25/Air_Quality_Study1.html",
            "relUrl": "/pollution/altair/pandas/visualization/2020/11/25/Air_Quality_Study1.html",
            "date": " • Nov 25, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "http://kaushalbundel.page/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am Kaushal Bundel. I enjoy solving problems and this blog is an attempt to share these observation with wider world. . Please drop a mail to kaushalbundel@gmail.com .",
          "url": "http://kaushalbundel.page/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "http://kaushalbundel.page/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}